{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8672423c",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-21ef9330a379>, line 172)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-21ef9330a379>\"\u001b[1;36m, line \u001b[1;32m172\u001b[0m\n\u001b[1;33m    D_real_loss =\u001b[0m\n\u001b[1;37m                  ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# CMU 16-726 Learning-Based Image Synthesis / Spring 2022, Assignment 3\n",
    "#\n",
    "# The code base is based on the great work from CSC 321, U Toronto\n",
    "# https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip\n",
    "# This is the main training file for the first part of the assignment.\n",
    "#\n",
    "# Usage:\n",
    "# ======\n",
    "#    To train with the default hyperparamters (saves results to checkpoints_vanilla/ and samples_vanilla/):\n",
    "#       python vanilla_gan.py\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import imageio\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Numpy & Scipy imports\n",
    "import numpy as np\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "# Local imports\n",
    "import utils\n",
    "from data_loader import get_data_loader\n",
    "from models import DCGenerator, DCDiscriminator\n",
    "\n",
    "from diff_augment import DiffAugment\n",
    "policy = 'color,translation,cutout'\n",
    "\n",
    "\n",
    "SEED = 11\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def print_models(G, D):\n",
    "    \"\"\"Prints model information for the generators and discriminators.\n",
    "    \"\"\"\n",
    "    print(\"                    G                  \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(G)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"                    D                  \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(D)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "\n",
    "def create_model(opts):\n",
    "    \"\"\"Builds the generators and discriminators.\n",
    "    \"\"\"\n",
    "    G = DCGenerator(noise_size=opts.noise_size, conv_dim=opts.conv_dim)\n",
    "    D = DCDiscriminator(conv_dim=opts.conv_dim)\n",
    "\n",
    "    print_models(G, D)\n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        G.cuda()\n",
    "        D.cuda()\n",
    "        print('Models moved to GPU.')\n",
    "\n",
    "    return G, D\n",
    "\n",
    "\n",
    "def create_image_grid(array, ncols=None):\n",
    "    \"\"\"\n",
    "    \"\"\"\n",
    "    num_images, channels, cell_h, cell_w = array.shape\n",
    "\n",
    "    if not ncols:\n",
    "        ncols = int(np.sqrt(num_images))\n",
    "    nrows = int(np.math.floor(num_images / float(ncols)))\n",
    "    result = np.zeros((cell_h*nrows, cell_w*ncols, channels), dtype=array.dtype)\n",
    "    for i in range(0, nrows):\n",
    "        for j in range(0, ncols):\n",
    "            result[i*cell_h:(i+1)*cell_h, j*cell_w:(j+1)*cell_w, :] = array[i*ncols+j].transpose(1, 2, 0)\n",
    "\n",
    "    if channels == 1:\n",
    "        result = result.squeeze()\n",
    "    return result\n",
    "\n",
    "\n",
    "def checkpoint(iteration, G, D, opts):\n",
    "    \"\"\"Saves the parameters of the generator G and discriminator D.\n",
    "    \"\"\"\n",
    "    G_path = os.path.join(opts.checkpoint_dir, 'G_iter%d.pkl' % iteration)\n",
    "    D_path = os.path.join(opts.checkpoint_dir, 'D_iter%d.pkl' % iteration)\n",
    "    torch.save(G.state_dict(), G_path)\n",
    "    torch.save(D.state_dict(), D_path)\n",
    "\n",
    "\n",
    "def save_samples(G, fixed_noise, iteration, opts):\n",
    "    generated_images = G(fixed_noise)\n",
    "    generated_images = utils.to_data(generated_images)\n",
    "\n",
    "    grid = create_image_grid(generated_images)\n",
    "\n",
    "    # merged = merge_images(X, fake_Y, opts)\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}.png'.format(iteration))\n",
    "    imageio.imwrite(path, grid)\n",
    "    print('Saved {}'.format(path))\n",
    "\n",
    "\n",
    "def save_images(images, iteration, opts, name):\n",
    "    grid = create_image_grid(utils.to_data(images))\n",
    "\n",
    "    path = os.path.join(opts.sample_dir, '{:s}-{:06d}.png'.format(name, iteration))\n",
    "    imageio.imwrite(path, grid)\n",
    "    print('Saved {}'.format(path))\n",
    "\n",
    "\n",
    "def sample_noise(dim):\n",
    "    \"\"\"\n",
    "    Generate a PyTorch Variable of uniform random noise.\n",
    "\n",
    "    Input:\n",
    "    - batch_size: Integer giving the batch size of noise to generate.\n",
    "    - dim: Integer giving the dimension of noise to generate.\n",
    "\n",
    "    Output:\n",
    "    - A PyTorch Variable of shape (batch_size, dim, 1, 1) containing uniform\n",
    "      random noise in the range (-1, 1).\n",
    "    \"\"\"\n",
    "    return utils.to_var(torch.rand(batch_size, dim) * 2 - 1).unsqueeze(2).unsqueeze(3)\n",
    "\n",
    "\n",
    "def training_loop(train_dataloader, opts):\n",
    "    \"\"\"Runs the training loop.\n",
    "        * Saves checkpoints every opts.checkpoint_every iterations\n",
    "        * Saves generated samples every opts.sample_every iterations\n",
    "    \"\"\"\n",
    "\n",
    "    # Create generators and discriminators\n",
    "    G, D = create_model(opts)\n",
    "\n",
    "    # Create optimizers for the generators and discriminators\n",
    "    g_optimizer = optim.Adam(G.parameters(), opts.lr, [opts.beta1, opts.beta2])\n",
    "    d_optimizer = optim.Adam(D.parameters(), opts.lr, [opts.beta1, opts.beta2])\n",
    "\n",
    "    # Generate fixed noise for sampling from the generator\n",
    "    fixed_noise = sample_noise(opts.noise_size)  # batch_size x noise_size x 1 x 1\n",
    "\n",
    "    iteration = 1\n",
    "\n",
    "    total_train_iters = opts.num_epochs * len(train_dataloader)\n",
    "\n",
    "    for epoch in range(opts.num_epochs):\n",
    "\n",
    "        for batch in train_dataloader:\n",
    "\n",
    "            real_images, labels = batch\n",
    "            real_images, labels = utils.to_var(real_images), utils.to_var(labels).long().squeeze()\n",
    "\n",
    "            ################################################\n",
    "            ###         TRAIN THE DISCRIMINATOR         ####\n",
    "            ################################################\n",
    "\n",
    "            # FILL THIS IN\n",
    "            # 1. Compute the discriminator loss on real images\n",
    "            # D_real_loss = torch.mean((D(real_images) - 1)**2)\n",
    "            D_real_loss = \n",
    "\n",
    "            # 2. Sample noise\n",
    "            noise = \n",
    "\n",
    "            # 3. Generate fake images from the noise\n",
    "            fake_images = \n",
    "\n",
    "            # 4. Compute the discriminator loss on the fake images\n",
    "            # D_fake_loss = torch.mean((D(fake_images.detach())) ** 2)\n",
    "            D_fake_loss = \n",
    "\n",
    "            D_total_loss = \n",
    "\n",
    "            # update the discriminator D\n",
    "            d_optimizer.zero_grad()\n",
    "            D_total_loss.backward()\n",
    "            d_optimizer.step()\n",
    "\n",
    "            ###########################################\n",
    "            ###          TRAIN THE GENERATOR        ###\n",
    "            ###########################################\n",
    "\n",
    "            # FILL THIS IN\n",
    "            # 1. Sample noise\n",
    "            noise = \n",
    "\n",
    "            # 2. Generate fake images from the noise\n",
    "            fake_images = \n",
    "\n",
    "            # 3. Compute the generator loss\n",
    "            G_loss = \n",
    "\n",
    "            # update the generator G\n",
    "            g_optimizer.zero_grad()\n",
    "            G_loss.backward()\n",
    "            g_optimizer.step()\n",
    "\n",
    "            # Print the log info\n",
    "            if iteration % opts.log_step == 0:\n",
    "                print('Iteration [{:4d}/{:4d}] | D_real_loss: {:6.4f} | D_fake_loss: {:6.4f} | G_loss: {:6.4f}'.format(\n",
    "                       iteration, total_train_iters, D_real_loss.item(), D_fake_loss.item(), G_loss.item()))\n",
    "                logger.add_scalar('D/fake', D_fake_loss, iteration)\n",
    "                logger.add_scalar('D/real', D_real_loss, iteration)\n",
    "                logger.add_scalar('D/total', D_total_loss, iteration)\n",
    "                logger.add_scalar('G/total', G_loss, iteration)\n",
    "\n",
    "            # Save the generated samples\n",
    "            if iteration % opts.sample_every == 0:\n",
    "                save_samples(G, fixed_noise, iteration, opts)\n",
    "                save_images(real_images, iteration, opts, 'real')\n",
    "\n",
    "            # Save the model parameters\n",
    "            if iteration % opts.checkpoint_every == 0:\n",
    "                checkpoint(iteration, G, D, opts)\n",
    "\n",
    "            iteration += 1\n",
    "\n",
    "\n",
    "def main(opts):\n",
    "    \"\"\"Loads the data, creates checkpoint and sample directories, and starts the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create a dataloader for the training images\n",
    "    dataloader = get_data_loader(opts.data, opts)\n",
    "\n",
    "    # Create checkpoint and sample directories\n",
    "    utils.create_dir(opts.checkpoint_dir)\n",
    "    utils.create_dir(opts.sample_dir)\n",
    "\n",
    "    training_loop(dataloader, opts)\n",
    "\n",
    "\n",
    "def create_parser():\n",
    "    \"\"\"Creates a parser for command-line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Model hyper-parameters\n",
    "    parser.add_argument('--image_size', type=int, default=64, help='The side length N to convert images to NxN.')\n",
    "    parser.add_argument('--conv_dim', type=int, default=32)\n",
    "    parser.add_argument('--noise_size', type=int, default=100)\n",
    "\n",
    "    # Training hyper-parameters\n",
    "    parser.add_argument('--num_epochs', type=int, default=500)\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='The number of images in a batch.')\n",
    "    parser.add_argument('--num_workers', type=int, default=0, help='The number of threads to use for the DataLoader.')\n",
    "    parser.add_argument('--lr', type=float, default=0.0002, help='The learning rate (default 0.0002)')\n",
    "    parser.add_argument('--beta1', type=float, default=0.5)\n",
    "    parser.add_argument('--beta2', type=float, default=0.999)\n",
    "\n",
    "    # Data sources\n",
    "    parser.add_argument('--data', type=str, default='cat/grumpifyBprocessed', help='The folder of the training dataset.')\n",
    "    parser.add_argument('--data_preprocess', type=str, default='deluxe', help='data preprocess scheme [basic|deluxe]')\n",
    "    parser.add_argument('--ext', type=str, default='*.png', help='Choose the file type of images to generate.')\n",
    "\n",
    "    # Directories and checkpoint/sample iterations\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='./checkpoints_vanilla')\n",
    "    parser.add_argument('--sample_dir', type=str, default='./vanilla')\n",
    "    parser.add_argument('--log_step', type=int , default=10)\n",
    "    parser.add_argument('--sample_every', type=int , default=200)\n",
    "    parser.add_argument('--checkpoint_every', type=int , default=400)\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = create_parser()\n",
    "    opts = parser.parse_args()\n",
    "\n",
    "    batch_size = opts.batch_size\n",
    "    opts.sample_dir = os.path.join('output/', opts.sample_dir,\n",
    "                                   '%s_%s' % (os.path.basename(opts.data), opts.data_preprocess))\n",
    "    if opts.use_diffaug:\n",
    "        opts.sample_dir += '_diffaug'\n",
    "\n",
    "    if os.path.exists(opts.sample_dir):\n",
    "        cmd = 'rm %s/*' % opts.sample_dir\n",
    "        os.system(cmd)\n",
    "    logger = SummaryWriter(opts.sample_dir)\n",
    "    print(opts)\n",
    "    main(opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e8ed91c1",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-1-5c6a04923b35>, line 204)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-1-5c6a04923b35>\"\u001b[1;36m, line \u001b[1;32m204\u001b[0m\n\u001b[1;33m    D_X_loss =\u001b[0m\n\u001b[1;37m               ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "# CMU 16-726 Learning-Based Image Synthesis / Spring 2022, Assignment 3\n",
    "#\n",
    "# The code base is based on the great work from CSC 321, U Toronto\n",
    "# https://www.cs.toronto.edu/~rgrosse/courses/csc321_2018/assignments/a4-code.zip\n",
    "# This is the main training file for the second part of the assignment.\n",
    "#\n",
    "# Usage:\n",
    "# ======\n",
    "#    To train with the default hyperparamters (saves results to samples_cyclegan/):\n",
    "#       python cycle_gan.py\n",
    "#\n",
    "#    To train with cycle consistency loss (saves results to samples_cyclegan_cycle/):\n",
    "#       python cycle_gan.py --use_cycle_consistency_loss\n",
    "#\n",
    "#\n",
    "#    For optional experimentation:\n",
    "#    -----------------------------\n",
    "#    If you have a powerful computer (ideally with a GPU), then you can obtain better results by\n",
    "#    increasing the number of filters used in the generator and/or discriminator, as follows:\n",
    "#      python cycle_gan.py --g_conv_dim=64 --d_conv_dim=64\n",
    "\n",
    "import argparse\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "import imageio\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# Torch imports\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "\n",
    "# Numpy & Scipy imports\n",
    "import numpy as np\n",
    "\n",
    "# Local imports\n",
    "import utils\n",
    "from data_loader import get_data_loader\n",
    "from models import CycleGenerator, DCDiscriminator, PatchDiscriminator\n",
    "\n",
    "from diff_augment import DiffAugment\n",
    "policy = 'color,translation,cutout' # If your dataset is as small as ours (e.g.,\n",
    "\n",
    "SEED = 11\n",
    "\n",
    "# Set the random seed manually for reproducibility.\n",
    "np.random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed(SEED)\n",
    "\n",
    "\n",
    "def print_models(G_XtoY, G_YtoX, D_X, D_Y):\n",
    "    \"\"\"Prints model information for the generators and discriminators.\n",
    "    \"\"\"\n",
    "    print(\"                 G_XtoY                \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(G_XtoY)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"                 G_YtoX                \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(G_YtoX)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"                  D_X                  \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(D_X)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "    print(\"                  D_Y                  \")\n",
    "    print(\"---------------------------------------\")\n",
    "    print(D_Y)\n",
    "    print(\"---------------------------------------\")\n",
    "\n",
    "\n",
    "def create_model(opts):\n",
    "    \"\"\"Builds the generators and discriminators.\n",
    "    \"\"\"\n",
    "    model_dict = {'cycle': CycleGenerator}\n",
    "    G_XtoY = model_dict[opts.gen](conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights, norm=opts.norm)\n",
    "    G_YtoX = model_dict[opts.gen](conv_dim=opts.g_conv_dim, init_zero_weights=opts.init_zero_weights, norm=opts.norm)\n",
    "\n",
    "    model_dict = {'dc': DCDiscriminator, 'patch': PatchDiscriminator}\n",
    "    D_X = model_dict[opts.disc](conv_dim=opts.d_conv_dim, norm=opts.norm)\n",
    "    D_Y = model_dict[opts.disc](conv_dim=opts.d_conv_dim, norm=opts.norm)\n",
    "    print_models(G_XtoY, G_YtoX, D_X, D_Y)\n",
    "\n",
    "    # TODO: B&W add your own initialization here\n",
    "    # \n",
    "\n",
    "    if torch.cuda.is_available():\n",
    "        G_XtoY.cuda()\n",
    "        G_YtoX.cuda()\n",
    "        D_X.cuda()\n",
    "        D_Y.cuda()\n",
    "        print('Models moved to GPU.')\n",
    "\n",
    "    return G_XtoY, G_YtoX, D_X, D_Y\n",
    "\n",
    "\n",
    "def checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts):\n",
    "    \"\"\"Saves the parameters of both generators G_YtoX, G_XtoY and discriminators D_X, D_Y.\n",
    "    \"\"\"\n",
    "    G_XtoY_path = os.path.join(opts.checkpoint_dir, 'G_XtoY_iter%d.pkl' % iteration)\n",
    "    G_YtoX_path = os.path.join(opts.checkpoint_dir, 'G_YtoX_iter%d.pkl' % iteration)\n",
    "    D_X_path = os.path.join(opts.checkpoint_dir, 'D_X_iter%d.pkl' % iteration)\n",
    "    D_Y_path = os.path.join(opts.checkpoint_dir, 'D_Y_iter%d.pkl' % iteration)\n",
    "    torch.save(G_XtoY.state_dict(), G_XtoY_path)\n",
    "    torch.save(G_YtoX.state_dict(), G_YtoX_path)\n",
    "    torch.save(D_X.state_dict(), D_X_path)\n",
    "    torch.save(D_Y.state_dict(), D_Y_path)\n",
    "\n",
    "\n",
    "def merge_images(sources, targets, opts, k=10):\n",
    "    \"\"\"Creates a grid consisting of pairs of columns, where the first column in\n",
    "    each pair contains images source images and the second column in each pair\n",
    "    contains images generated by the CycleGAN from the corresponding images in\n",
    "    the first column.\n",
    "    \"\"\"\n",
    "    _, _, h, w = sources.shape\n",
    "    row = int(np.sqrt(opts.batch_size))\n",
    "    merged = np.zeros([3, row*h, row*w*2])\n",
    "    for idx, (s, t) in enumerate(zip(sources, targets)):\n",
    "        i = idx // row\n",
    "        j = idx % row\n",
    "        merged[:, i*h:(i+1)*h, (j*2)*h:(j*2+1)*h] = s\n",
    "        merged[:, i*h:(i+1)*h, (j*2+1)*h:(j*2+2)*h] = t\n",
    "    return merged.transpose(1, 2, 0)\n",
    "\n",
    "\n",
    "def save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts):\n",
    "    \"\"\"Saves samples from both generators X->Y and Y->X.\n",
    "    \"\"\"\n",
    "    fake_X = G_YtoX(fixed_Y)\n",
    "    fake_Y = G_XtoY(fixed_X)\n",
    "\n",
    "    X, fake_X = utils.to_data(fixed_X), utils.to_data(fake_X)\n",
    "    Y, fake_Y = utils.to_data(fixed_Y), utils.to_data(fake_Y)\n",
    "\n",
    "    merged = merge_images(X, fake_Y, opts)\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-X-Y.png'.format(iteration))\n",
    "    imageio.imwrite(path, merged)\n",
    "    print('Saved {}'.format(path))\n",
    "\n",
    "    merged = merge_images(Y, fake_X, opts)\n",
    "    path = os.path.join(opts.sample_dir, 'sample-{:06d}-Y-X.png'.format(iteration))\n",
    "    imageio.imwrite(path, merged)\n",
    "    print('Saved {}'.format(path))\n",
    "\n",
    "\n",
    "def training_loop(dataloader_X, dataloader_Y, opts):\n",
    "    \"\"\"Runs the training loop.\n",
    "        * Saves checkpoint every opts.checkpoint_every iterations\n",
    "        * Saves generated samples every opts.sample_every iterations\n",
    "    \"\"\"\n",
    "\n",
    "    # Create generators and discriminators\n",
    "    G_XtoY, G_YtoX, D_X, D_Y = create_model(opts)\n",
    "\n",
    "    g_params = list(G_XtoY.parameters()) + list(G_YtoX.parameters())  # Get generator parameters\n",
    "    d_params = list(D_X.parameters()) + list(D_Y.parameters())  # Get discriminator parameters\n",
    "\n",
    "    # Create optimizers for the generators and discriminators\n",
    "    g_optimizer = optim.Adam(g_params, opts.lr, [opts.beta1, opts.beta2])\n",
    "    d_optimizer = optim.Adam(d_params, opts.lr, [opts.beta1, opts.beta2])\n",
    "\n",
    "    iter_X = iter(dataloader_X)\n",
    "    iter_Y = iter(dataloader_Y)\n",
    "\n",
    "\n",
    "    # Get some fixed data from domains X and Y for sampling. These are images that are held\n",
    "    # constant throughout training, that allow us to inspect the model's performance.\n",
    "    fixed_X = utils.to_var(iter_X.next()[0])\n",
    "    fixed_Y = utils.to_var(iter_Y.next()[0])\n",
    "\n",
    "    iter_per_epoch = min(len(iter_X), len(iter_Y))\n",
    "\n",
    "    for iteration in range(1, opts.train_iters+1):\n",
    "\n",
    "        # Reset data_iter for each epoch\n",
    "        if iteration % iter_per_epoch == 0:\n",
    "            iter_X = iter(dataloader_X)\n",
    "            iter_Y = iter(dataloader_Y)\n",
    "\n",
    "        images_X, labels_X = iter_X.next()\n",
    "        images_X, labels_X = utils.to_var(images_X), utils.to_var(labels_X).long().squeeze()\n",
    "\n",
    "        images_Y, labels_Y = iter_Y.next()\n",
    "        images_Y, labels_Y = utils.to_var(images_Y), utils.to_var(labels_Y).long().squeeze()\n",
    "\n",
    "\n",
    "        # ============================================\n",
    "        #            TRAIN THE DISCRIMINATORS\n",
    "        # ============================================\n",
    "\n",
    "        #########################################\n",
    "        ##             FILL THIS IN            ##\n",
    "        #########################################\n",
    "\n",
    "        # 1. Compute the discriminator losses on real images\n",
    "        D_X_loss = \n",
    "        D_Y_loss = \n",
    "\n",
    "        d_real_loss = D_X_loss + D_Y_loss\n",
    "\n",
    "        # 2. Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_X = \n",
    "\n",
    "        # 3. Compute the loss for D_X\n",
    "        D_X_loss = \n",
    "\n",
    "        # 4. Generate fake images that look like domain Y based on real images in domain X\n",
    "        fake_Y = \n",
    "\n",
    "        # 5. Compute the loss for D_Y\n",
    "        D_Y_loss = \n",
    "\n",
    "        d_fake_loss = D_X_loss + D_Y_loss\n",
    "\n",
    "        # sum up the losses and update D_X and D_Y\n",
    "        d_optimizer.zero_grad()\n",
    "        d_total_loss = d_real_loss + d_fake_loss\n",
    "        d_total_loss.backward()\n",
    "        d_optimizer.step()\n",
    "\n",
    "        # plot the losses in tensorboard\n",
    "        logger.add_scalar('D/XY/real', D_X_loss, iteration)\n",
    "        logger.add_scalar('D/YX/real', D_Y_loss, iteration)\n",
    "        logger.add_scalar('D/XY/fake', D_X_loss, iteration)\n",
    "        logger.add_scalar('D/YX/fake', D_Y_loss, iteration)\n",
    "\n",
    "\n",
    "        # =========================================\n",
    "        #            TRAIN THE GENERATORS\n",
    "        # =========================================\n",
    "\n",
    "\n",
    "        #########################################\n",
    "        ##    FILL THIS IN: Y--X-->Y CYCLE     ##\n",
    "        #########################################\n",
    "\n",
    "        # 1. Generate fake images that look like domain X based on real images in domain Y\n",
    "        fake_X = \n",
    "\n",
    "        # 2. Compute the generator loss based on domain X\n",
    "        g_loss = \n",
    "        logger.add_scalar('G/XY/fake', g_loss, iteration)\n",
    "\n",
    "        if opts.use_cycle_consistency_loss:\n",
    "            # 3. Compute the cycle consistency loss (the reconstruction loss)\n",
    "            cycle_consistency_loss = \n",
    "\n",
    "            g_loss += opts.lambda_cycle * cycle_consistency_loss\n",
    "            logger.add_scalar('G/XY/cycle', opts.lambda_cycle * cycle_consistency_loss, iteration)\n",
    "\n",
    "        #########################################\n",
    "        ##    FILL THIS IN: X--Y-->X CYCLE     ##\n",
    "        #########################################\n",
    "\n",
    "        # 1. Generate fake images that look like domain Y based on real images in domain X\n",
    "        fake_Y = \n",
    "\n",
    "        # 2. Compute the generator loss based on domain Y\n",
    "        g_loss += \n",
    "        logger.add_scalar('G/YX/fake', g_loss, iteration)\n",
    "\n",
    "        if opts.use_cycle_consistency_loss:\n",
    "            # 3. Compute the cycle consistency loss (the reconstruction loss)\n",
    "            cycle_consistency_loss = \n",
    "\n",
    "            g_loss += opts.lambda_cycle * cycle_consistency_loss\n",
    "            logger.add_scalar('G/YX/cycle', cycle_consistency_loss, iteration)\n",
    "\n",
    "        # backprop the aggregated g losses and update G_XtoY and G_YtoX\n",
    "        g_optimizer.zero_grad()\n",
    "        g_loss.backward()\n",
    "        g_optimizer.step()\n",
    "\n",
    "        # Print the log info\n",
    "        if iteration % opts.log_step == 0:\n",
    "            print('Iteration [{:5d}/{:5d}] | d_real_loss: {:6.4f} | d_Y_loss: {:6.4f} | d_X_loss: {:6.4f} | '\n",
    "                  'd_fake_loss: {:6.4f} | g_loss: {:6.4f}'.format(\n",
    "                    iteration, opts.train_iters, d_real_loss.item(), D_Y_loss.item(),\n",
    "                    D_X_loss.item(), d_fake_loss.item(), g_loss.item()))\n",
    "\n",
    "        # Save the generated samples\n",
    "        if iteration % opts.sample_every == 0:\n",
    "            save_samples(iteration, fixed_Y, fixed_X, G_YtoX, G_XtoY, opts)\n",
    "\n",
    "        # Save the model parameters\n",
    "        if iteration % opts.checkpoint_every == 0:\n",
    "            checkpoint(iteration, G_XtoY, G_YtoX, D_X, D_Y, opts)\n",
    "\n",
    "\n",
    "def main(opts):\n",
    "    \"\"\"Loads the data, creates checkpoint and sample directories, and starts the training loop.\n",
    "    \"\"\"\n",
    "\n",
    "    # Create  dataloaders for images from the two domains X and Y\n",
    "    dataloader_X= get_data_loader(opts.X, opts=opts)\n",
    "    dataloader_Y = get_data_loader(opts.Y, opts=opts)\n",
    "\n",
    "    # Create checkpoint and sample directories\n",
    "    utils.create_dir(opts.checkpoint_dir)\n",
    "    utils.create_dir(opts.sample_dir)\n",
    "\n",
    "    # Start training\n",
    "    training_loop(dataloader_X, dataloader_Y, opts)\n",
    "\n",
    "\n",
    "def print_opts(opts):\n",
    "    \"\"\"Prints the values of all command-line arguments.\n",
    "    \"\"\"\n",
    "    print('=' * 80)\n",
    "    print('Opts'.center(80))\n",
    "    print('-' * 80)\n",
    "    for key in opts.__dict__:\n",
    "        if opts.__dict__[key]:\n",
    "            print('{:>30}: {:<30}'.format(key, opts.__dict__[key]).center(80))\n",
    "    print('=' * 80)\n",
    "\n",
    "\n",
    "def create_parser():\n",
    "    \"\"\"Creates a parser for command-line arguments.\n",
    "    \"\"\"\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    # Model hyper-parameters\n",
    "    parser.add_argument('--image_size', type=int, default=64, help='The side length N to convert images to NxN.')\n",
    "    parser.add_argument('--disc', type=str, default='dc', help='Choose which discriminator to use. choices:[dc|patch]')\n",
    "    parser.add_argument('--gen', type=str, default='cycle')\n",
    "    parser.add_argument('--g_conv_dim', type=int, default=32)\n",
    "    parser.add_argument('--d_conv_dim', type=int, default=32)\n",
    "    parser.add_argument('--norm', type=str, default='instance')\n",
    "    parser.add_argument('--use_cycle_consistency_loss', action='store_true', default=False, help='Choose whether to include the cycle consistency term in the loss.')\n",
    "    parser.add_argument('--init_zero_weights', action='store_true', default=False, help='Choose whether to initialize the generator conv weights to 0 (implements the identity function).')\n",
    "    parser.add_argument('--init_type', type=str, default='naive')\n",
    "\n",
    "    # Training hyper-parameters\n",
    "    parser.add_argument('--train_iters', type=int, default=10000, help='The number of training iterations to run (you can Ctrl-C out earlier if you want).')\n",
    "    parser.add_argument('--batch_size', type=int, default=16, help='The number of images in a batch.')\n",
    "    parser.add_argument('--num_workers', type=int, default=0, help='The number of threads to use for the DataLoader.')\n",
    "    parser.add_argument('--lr', type=float, default=0.0003, help='The learning rate (default 0.0003)')\n",
    "    parser.add_argument('--beta1', type=float, default=0.5)\n",
    "    parser.add_argument('--beta2', type=float, default=0.999)\n",
    "    parser.add_argument('--lambda_cycle', type=float, default=10)\n",
    "\n",
    "    # Data sources\n",
    "    parser.add_argument('--X', type=str, default='cat/grumpifyAprocessed', help='Choose the type of images for domain X.')\n",
    "    parser.add_argument('--Y', type=str, default='cat/grumpifyBprocessed', help='Choose the type of images for domain Y.')\n",
    "    parser.add_argument('--ext', type=str, default='*.png', help='Choose the type of images for domain Y.')\n",
    "    parser.add_argument('--data_preprocess', type=str, default='deluxe', help='data preprocess scheme [basic|deluxe]')\n",
    "\n",
    "    # Saving directories and checkpoint/sample iterations\n",
    "    parser.add_argument('--checkpoint_dir', type=str, default='checkpoints_cyclegan')\n",
    "    parser.add_argument('--sample_dir', type=str, default='cyclegan')\n",
    "    parser.add_argument('--log_step', type=int , default=10)\n",
    "    parser.add_argument('--sample_every', type=int , default=100)\n",
    "    parser.add_argument('--checkpoint_every', type=int , default=800)\n",
    "\n",
    "    parser.add_argument('--gpu', type=str, default='0')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = create_parser()\n",
    "    opts = parser.parse_args()\n",
    "    os.environ['CUDA_VISIBLE_DEVICES'] = opts.gpu\n",
    "    opts.sample_dir = os.path.join('output/', opts.sample_dir,\n",
    "                                   '%s_%g' % (opts.X.split('/')[0], opts.lambda_cycle))\n",
    "    opts.sample_dir += '%s_%s_%s_%s_%s' % (opts.data_preprocess, opts.norm, opts.disc, opts.gen, opts.init_type)\n",
    "    if opts.use_cycle_consistency_loss:\n",
    "        opts.sample_dir += '_cycle'\n",
    "    if opts.use_diffaug:\n",
    "        opts.sample_dir += '_diffaug'\n",
    "\n",
    "    if os.path.exists(opts.sample_dir):\n",
    "        cmd = 'rm %s/*' % opts.sample_dir\n",
    "        os.system(cmd)\n",
    "\n",
    "    logger = SummaryWriter(opts.sample_dir)\n",
    "\n",
    "    print_opts(opts)\n",
    "    main(opts)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8583b726",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: Poisson blending. [-h] -q {toy,blend,mixed,color2gray}\n",
      "Poisson blending.: error: the following arguments are required: -q/--question\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[1;31mSystemExit\u001b[0m\u001b[1;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\envs\\ImageSynthesis\\lib\\site-packages\\IPython\\core\\interactiveshell.py:3351: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "# --------------------------------------------------------\n",
    "# Written by Yufei Ye and modified by Sheng-Yu Wang (https://github.com/JudyYe)\n",
    "# Convert from MATLAB code https://inst.eecs.berkeley.edu/~cs194-26/fa18/hw/proj3/gradient_starter.zip\n",
    "# --------------------------------------------------------\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import numpy as np\n",
    "import cv2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.sparse as sp\n",
    "from scipy.sparse.linalg import lsqr\n",
    "import time\n",
    "\n",
    "\n",
    "def toy_recon(image, loop=False):\n",
    "    imh, imw = image.shape\n",
    "    n_pixel = imh * imw\n",
    "    n_equation = n_pixel * 2 + 1  # x, y direction, (0,0) coord\n",
    "    im2var = np.arange(n_pixel).reshape((imh, imw)).astype(int)\n",
    "    A = sp.lil_matrix((n_equation, n_pixel))\n",
    "    b = np.zeros((n_equation, 1))\n",
    "    # loop method\n",
    "    if loop:\n",
    "        e = 0\n",
    "        # objective 1\n",
    "        for y in range(imh):\n",
    "            for x in range(imw - 1):\n",
    "                A[e, im2var[y, x + 1]] = 1\n",
    "                A[e, im2var[y, x]] = -1\n",
    "                b[e] = image[y, x + 1] - image[y, x]\n",
    "                e += 1\n",
    "        # objective 2\n",
    "        for y in range(imh - 1):\n",
    "            for x in range(imw):\n",
    "                A[e, im2var[y + 1, x]] = 1\n",
    "                A[e, im2var[y, x]] = -1\n",
    "                b[e] = image[y + 1, x] - image[y, x]\n",
    "                e += 1\n",
    "        # (0,0)\n",
    "        A[e, im2var[0, 0]] = 1\n",
    "        b[e] = image[0, 0]\n",
    "    # non-loop method\n",
    "    if not loop:\n",
    "        s = image\n",
    "        a1 = np.eye(n_pixel, n_pixel, dtype=int)\n",
    "        a2 = np.roll(a1, -1, axis=1)\n",
    "        a2[:, -1] = a1[:, -1]\n",
    "        a3 = (a2 - a1).reshape(n_pixel, imh, imw)\n",
    "        a3 = np.transpose(a3, (0, 2, 1)).reshape(n_pixel, n_pixel)\n",
    "        A[0:n_pixel, :] = a2 - a1\n",
    "        A[n_pixel:-1, :] = a3\n",
    "        A[-1, im2var[0, 0]] = 1\n",
    "\n",
    "        b1 = s\n",
    "        b2 = np.roll(b1, -1, axis=1)\n",
    "        b2[:, -1] = b1[:, -1]\n",
    "        b3 = np.roll(b1, -1, axis=0)\n",
    "        b3[-1, :] = b1[-1, :]\n",
    "        b[0:n_pixel] = (b2 - b1).reshape((n_pixel, 1))\n",
    "        b[n_pixel:-1] = (b3 - b1).reshape((n_pixel, 1))\n",
    "        b[-1] = s[0, 0]\n",
    "\n",
    "    v = lsqr(A.tocsr(), b)[0] * 255\n",
    "    output = v.reshape((imh, imw)).astype(int)\n",
    "    return output\n",
    "\n",
    "\n",
    "def poisson_blend(fg, mask, bg, mixed=False):\n",
    "    \"\"\"\n",
    "    Poisson Blending.\n",
    "    :param fg: (H, W, C) source texture / foreground object\n",
    "    :param mask: (H, W, 1)\n",
    "    :param bg: (H, W, C) target image / background\n",
    "    :return: (H, W, C)\n",
    "    \"\"\"\n",
    "    imh, imw, channels = min(fg.shape, bg.shape)\n",
    "    n_pixels = imh * imw\n",
    "    im2var = np.arange(n_pixels).reshape((imh, imw)).astype(int)\n",
    "    v_rgb = np.empty((imh, imw, channels), dtype=int)\n",
    "    A = sp.lil_matrix((n_pixels, n_pixels))\n",
    "    mask_index = np.where(mask == True)\n",
    "    n_mask = len(mask_index[0])\n",
    "\n",
    "    for c in range(channels):\n",
    "        b = np.zeros((n_pixels, 1))\n",
    "        for index in range(n_mask):\n",
    "            y = mask_index[0][index]\n",
    "            x = mask_index[1][index]\n",
    "            e = (y - 1) * imw + x\n",
    "            # construct A, only construct once because A is same for 3 channels\n",
    "            if c == 0:\n",
    "                if (y - 1) >= 0:\n",
    "                    if mask[y - 1, x]:\n",
    "                        A[e, im2var[y - 1, x]] = -1\n",
    "                if (y + 1) <= mask.shape[0] - 1:\n",
    "                    if mask[y + 1, x]:\n",
    "                        A[e, im2var[y + 1, x]] = -1\n",
    "                if (x - 1) >= 0:\n",
    "                    if mask[y, x - 1]:\n",
    "                        A[e, im2var[y, x - 1]] = -1\n",
    "                if (x + 1) <= mask.shape[1] - 1:\n",
    "                    if mask[y, x + 1]:\n",
    "                        A[e, im2var[y, x + 1]] = -1\n",
    "                # center point is 4\n",
    "                A[e, im2var[y, x]] = 4\n",
    "\n",
    "            # construct b, construct 3 times because b is different for r, g, b\n",
    "            # check the border\n",
    "            if y - 1 < 0:\n",
    "                fg_up = fg[y, x, c]\n",
    "                bg_up = 0\n",
    "            else:\n",
    "                fg_up = fg[y - 1, x, c]\n",
    "                bg_up = bg[y - 1, x, c]\n",
    "            if y + 1 > mask.shape[0] - 1:\n",
    "                fg_down = fg[y, x, c]\n",
    "                bg_down = 0\n",
    "            else:\n",
    "                fg_down = fg[y + 1, x, c]\n",
    "                bg_down = bg[y + 1, x, c]\n",
    "            if x - 1 < 0:\n",
    "                fg_left = fg[y, x, c]\n",
    "                bg_left = 0\n",
    "            else:\n",
    "                fg_left = fg[y, x - 1, c]\n",
    "                bg_left = bg[y, x - 1, c]\n",
    "            if x + 1 > mask.shape[1] - 1:\n",
    "                fg_right = fg[y, x, c]\n",
    "                bg_right = 0\n",
    "            else:\n",
    "                fg_right = fg[y, x + 1, c]\n",
    "                bg_right = bg[y, x + 1, c]\n",
    "\n",
    "            if mixed:\n",
    "                # mixed gradients\n",
    "                s1 = fg[y, x, c] - fg_up\n",
    "                t1 = bg[y, x, c] - bg_up\n",
    "                s2 = fg[y, x, c] - fg_down\n",
    "                t2 = bg[y, x, c] - bg_down\n",
    "                s3 = fg[y, x, c] - fg_left\n",
    "                t3 = bg[y, x, c] - bg_left\n",
    "                s4 = fg[y, x, c] - fg_right\n",
    "                t4 = bg[y, x, c] - bg_right\n",
    "\n",
    "                b[e] = 0\n",
    "                b[e] += s1 if abs(s1) > abs(t1) else t1\n",
    "                b[e] += s2 if abs(s2) > abs(t2) else t2\n",
    "                b[e] += s3 if abs(s3) > abs(t3) else t3\n",
    "                b[e] += s4 if abs(s4) > abs(t4) else t4\n",
    "            else:\n",
    "                b[e] = 4 * fg[y, x, c] - fg_up - fg_down - fg_right - fg_left\n",
    "\n",
    "            if y - 1 >= 0:\n",
    "                if not mask[y - 1, x]:\n",
    "                    b[e] += bg_up\n",
    "            if y + 1 <= mask.shape[0] - 1:\n",
    "                if not mask[y + 1, x]:\n",
    "                    b[e] += bg_down\n",
    "            if x - 1 >= 0:\n",
    "                if not mask[y, x - 1]:\n",
    "                    b[e] += bg_left\n",
    "            if x + 1 <= mask.shape[1] - 1:\n",
    "                if not mask[y, x + 1]:\n",
    "                    b[e] += bg_right\n",
    "\n",
    "        # calculate lsq, only the mask area is what we want\n",
    "        v = lsqr(A.tocsr(), b)[0] * 255\n",
    "        v_rgb[:, :, c] = v.reshape((imh, imw)).astype(int)\n",
    "\n",
    "    return v_rgb / 255. * mask + bg * (1 - mask)\n",
    "\n",
    "def mixed_blend(fg, mask, bg):\n",
    "    \"\"\"EC: Mix gradient of source and target\"\"\"\n",
    "    return poisson_blend(fg, mask, bg, mixed=True)\n",
    "\n",
    "\n",
    "def color2gray(rgb_image):\n",
    "    \"\"\"Naive conversion from an RGB image to a gray image.\"\"\"\n",
    "    return cv2.cvtColor(rgb_image, cv2.COLOR_RGB2GRAY)\n",
    "\n",
    "\n",
    "def mixed_grad_color2gray(rgb_image):\n",
    "    \"\"\"EC: Convert an RGB image to gray image using mixed gradients.\"\"\"\n",
    "    img = cv2.cvtColor(rgb_image, cv2.COLOR_RGB2HSV)\n",
    "    s = img[:, :, 1].reshape((img.shape[0], img.shape[1], 1)) / 255.\n",
    "    v = img[:, :, 2].reshape((img.shape[0], img.shape[1], 1)) / 255.\n",
    "    mask = np.ones_like(img[:, :, 0]).reshape((img.shape[0], img.shape[1], 1))\n",
    "    # mask[0, :] = 0\n",
    "    # mask[-1, :] = 0\n",
    "    # mask[:, 0] = 0\n",
    "    # mask[:, -1] = 0\n",
    "    # mask = mask > 0\n",
    "    output = poisson_blend(s, mask, v, mixed=True)\n",
    "    return output\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser(\"Poisson blending.\")\n",
    "    parser.add_argument(\"-q\", \"--question\", required=True, choices=[\"toy\", \"blend\", \"mixed\", \"color2gray\"])\n",
    "    args, _ = parser.parse_known_args()\n",
    "\n",
    "    # Example script: python proj2_starter.py -q toy\n",
    "    if args.question == \"toy\":\n",
    "        image = imageio.imread('./data/toy_problem.png') / 255.\n",
    "\n",
    "        timer = time.time()\n",
    "        image_hat = toy_recon(image, loop=True)\n",
    "        print(\"Time used:\" + str(time.time() - timer))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(image, cmap='gray')\n",
    "        plt.title('Input')\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(image_hat, cmap='gray')\n",
    "        plt.title('Output')\n",
    "        plt.show()\n",
    "\n",
    "    # Example script: python proj2_starter.py -q blend -s data/source_01_newsource.png -t data/target_01.jpg -m data/target_01_mask.png\n",
    "    if args.question == \"blend\":\n",
    "        parser.add_argument(\"-s\", \"--source\", required=True)\n",
    "        parser.add_argument(\"-t\", \"--target\", required=True)\n",
    "        parser.add_argument(\"-m\", \"--mask\", required=True)\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        # after alignment (masking_code.py)\n",
    "        ratio = 1\n",
    "        fg = cv2.resize(imageio.imread(args.source), (0, 0), fx=ratio, fy=ratio)\n",
    "        bg = cv2.resize(imageio.imread(args.target), (0, 0), fx=ratio, fy=ratio)\n",
    "        mask = cv2.resize(imageio.imread(args.mask), (0, 0), fx=ratio, fy=ratio)\n",
    "\n",
    "        fg = fg / 255.\n",
    "        bg = bg / 255.\n",
    "        mask = (mask.sum(axis=2, keepdims=True) > 0)\n",
    "\n",
    "        timer = time.time()\n",
    "        blend_img = poisson_blend(fg, mask, bg)\n",
    "        print(\"Time used:\" + str(time.time() - timer))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(fg * mask + bg * (1 - mask))\n",
    "        plt.title('Naive Blend')\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(blend_img)\n",
    "        plt.title('Poisson Blend')\n",
    "        plt.show()\n",
    "\n",
    "    if args.question == \"mixed\":\n",
    "        parser.add_argument(\"-s\", \"--source\", required=True)\n",
    "        parser.add_argument(\"-t\", \"--target\", required=True)\n",
    "        parser.add_argument(\"-m\", \"--mask\", required=True)\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        # after alignment (masking_code.py)\n",
    "        ratio = 1\n",
    "        fg = cv2.resize(imageio.imread(args.source), (0, 0), fx=ratio, fy=ratio)\n",
    "        bg = cv2.resize(imageio.imread(args.target), (0, 0), fx=ratio, fy=ratio)\n",
    "        mask = cv2.resize(imageio.imread(args.mask), (0, 0), fx=ratio, fy=ratio)\n",
    "\n",
    "        fg = fg / 255.\n",
    "        bg = bg / 255.\n",
    "        mask = (mask.sum(axis=2, keepdims=True) > 0)\n",
    "        timer = time.time()\n",
    "        blend_img = mixed_blend(fg, mask, bg)\n",
    "        print(\"Time used:\" + str(time.time() - timer))\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(fg * mask + bg * (1 - mask))\n",
    "        plt.title('Naive Blend')\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(blend_img)\n",
    "        plt.title('Mixed Blend')\n",
    "        plt.show()\n",
    "\n",
    "    if args.question == \"color2gray\":\n",
    "        parser.add_argument(\"-s\", \"--source\", required=True)\n",
    "        args = parser.parse_args()\n",
    "\n",
    "        rgb_image = imageio.imread(args.source)\n",
    "        gray_image = color2gray(rgb_image)\n",
    "        timer = time.time()\n",
    "        mixed_grad_img = mixed_grad_color2gray(rgb_image)\n",
    "        print(\"Time used:\" + str(time.time() - timer))\n",
    "\n",
    "        plt.subplot(121)\n",
    "        plt.imshow(gray_image, cmap='gray')\n",
    "        plt.title('rgb2gray')\n",
    "        plt.subplot(122)\n",
    "        plt.imshow(mixed_grad_img, cmap='gray')\n",
    "        plt.title('mixed gradient')\n",
    "        plt.show()\n",
    "\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3509bc63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# CMU 16-726 Learning-Based Image Synthesis / Spring 2022, Assignment 3\n",
    "# The code is based on this paper:\n",
    "# Differentiable Augmentation for Data-Efficient GAN Training\n",
    "# Shengyu Zhao, Zhijian Liu, Ji Lin, Jun-Yan Zhu, and Song Han\n",
    "# https://arxiv.org/pdf/2006.10738\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "def DiffAugment(x, policy='', channels_first=True):\n",
    "    if policy:\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 3, 1, 2)\n",
    "        for p in policy.split(','):\n",
    "            for f in AUGMENT_FNS[p]:\n",
    "                x = f(x)\n",
    "        if not channels_first:\n",
    "            x = x.permute(0, 2, 3, 1)\n",
    "        x = x.contiguous()\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_brightness(x):\n",
    "    x = x + (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) - 0.5)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_saturation(x):\n",
    "    x_mean = x.mean(dim=1, keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) * 2) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_contrast(x):\n",
    "    x_mean = x.mean(dim=[1, 2, 3], keepdim=True)\n",
    "    x = (x - x_mean) * (torch.rand(x.size(0), 1, 1, 1, dtype=x.dtype, device=x.device) + 0.5) + x_mean\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_translation(x, ratio=0.125):\n",
    "    shift_x, shift_y = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    translation_x = torch.randint(-shift_x, shift_x + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    translation_y = torch.randint(-shift_y, shift_y + 1, size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(2), dtype=torch.long, device=x.device),\n",
    "        torch.arange(x.size(3), dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + translation_x + 1, 0, x.size(2) + 1)\n",
    "    grid_y = torch.clamp(grid_y + translation_y + 1, 0, x.size(3) + 1)\n",
    "    x_pad = F.pad(x, [1, 1, 1, 1, 0, 0, 0, 0])\n",
    "    x = x_pad.permute(0, 2, 3, 1).contiguous()[grid_batch, grid_x, grid_y].permute(0, 3, 1, 2)\n",
    "    return x\n",
    "\n",
    "\n",
    "def rand_cutout(x, ratio=0.5):\n",
    "    cutout_size = int(x.size(2) * ratio + 0.5), int(x.size(3) * ratio + 0.5)\n",
    "    offset_x = torch.randint(0, x.size(2) + (1 - cutout_size[0] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    offset_y = torch.randint(0, x.size(3) + (1 - cutout_size[1] % 2), size=[x.size(0), 1, 1], device=x.device)\n",
    "    grid_batch, grid_x, grid_y = torch.meshgrid(\n",
    "        torch.arange(x.size(0), dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[0], dtype=torch.long, device=x.device),\n",
    "        torch.arange(cutout_size[1], dtype=torch.long, device=x.device),\n",
    "    )\n",
    "    grid_x = torch.clamp(grid_x + offset_x - cutout_size[0] // 2, min=0, max=x.size(2) - 1)\n",
    "    grid_y = torch.clamp(grid_y + offset_y - cutout_size[1] // 2, min=0, max=x.size(3) - 1)\n",
    "    mask = torch.ones(x.size(0), x.size(2), x.size(3), dtype=x.dtype, device=x.device)\n",
    "    mask[grid_batch, grid_x, grid_y] = 0\n",
    "    x = x * mask.unsqueeze(1)\n",
    "    return x\n",
    "\n",
    "\n",
    "AUGMENT_FNS = {\n",
    "    'color': [rand_brightness, rand_saturation, rand_contrast],\n",
    "    'translation': [rand_translation],\n",
    "    'cutout': [rand_cutout],\n",
    "}\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
