<!--
Dependecies:
jQuery JavaScript Library v1.4.2
http://jquery.com/
Copyright 2010, John Resig
Dual licensed under the MIT or GPL Version 2 licenses.
http://jquery.org/license
-----------------------------------------------------
Bootstrap
Copyright (c) 2011-2019 Twitter, Inc.
Copyright (c) 2011-2019 The Bootstrap Authors
https://github.com/twbs/bootstrap/blob/v4.3.1/LICENSE
-----------------------------------------------------
Vanta
https://github.com/tengbao/vanta
-----------------------------------------------------
Author: Joshua Cao | yuchenca@andrew.cmu.edu
Webiste: 16726 course website
-->
<!doctype html>
<html>

<head>
    <!-- icon -->
    <link rel="icon" href="./media/joshua.ico" type="image/x-icon">
    <link rel="shortcut icon" href="./media/joshua.ico" type="image/x-icon">
    <link rel="bookmark" href="./media/joshua.ico" type="image/x-icon">
    <!-- title -->
    <title>16726-Yuchenca</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <meta name="description" content="16726-Yuchenca">
    <meta name="keywords" lang="de" content="16726-Yuchenca">
    <!-- jQuery -->
    <script type="text/javascript" src="./js/jQuery-3.3.1.js"></script>
    <!-- Bootstrap -->
    <link type="text/css" href="./css/bootstrap.min.css" rel='stylesheet'>
    <script type="text/javascript" src="./js/bootstrap.min.js"></script>
    <!-- main -->
    <link type="text/css" href="./css/main.css" rel='stylesheet'>
    <script type="text/javascript" src="./js/main.js"></script>
    <!-- vanta -->
    <script type="text/javascript" src="./js/three.min.js"></script>
    <script type="text/javascript" src="./js/ring.min.js"></script>
    <script type="text/javascript" src="./js/cloud.min.js"></script>
</head>

<body>
    <div style="width:100%;height:100%">
        <!-- title -->
        <div  id="cloud">
            <div class="row">
                <div class="col-12 mx-12">
                    <div id="title" class="text-center">
                        <span id="title-top" class="font-weight-bold">16726 Learning-based Image Synthesis Spring 22</span>
                        <br>
                        <span id="title-bot">Joshua Cao | Carnegie Mellon University</span>
                    </div>
                </div>
            </div>
            <br><br>
            <!-- navigation -->
            <ul id="navigation" class="nav nav-tabs font-weight-bold text-center" role="tablist">
                <li class="nav-item">
                    <a class="nav-link" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="false">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="a1-tab" data-toggle="tab" href="#a1" role="tab" aria-controls="a1" aria-selected="false">Assignment1</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="a2-tab" data-toggle="tab" href="#a2" role="tab" aria-controls="a2" aria-selected="false">Assignment2</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link active" id="a3-tab" data-toggle="tab" href="#a3" role="tab" aria-controls="a3" aria-selected="true">Assignment3</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="a4-tab" data-toggle="tab" href="#a4" role="tab" aria-controls="a4" aria-selected="false">Assignment4</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="a5-tab" data-toggle="tab" href="#a5" role="tab" aria-controls="a5" aria-selected="false">Assignment5</a>
                </li>
            </ul>
        </div>
        <div class="tab-content container" id="TabContent">
            <div class="tab-pane fade" id="home" role="tabpanel" aria-labelledby="home-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>About Course</h2>
                        <hr class="col-xs-12 mr-4">
                        <p class="col-xs-12 mr-4">
                            <a class="underline" href="https://learning-image-synthesis.github.io/sp22/" target="_blank">16-726 Learning-Based Image Synthesis / Spring 2022</a> is led by <a class="underline" href="https://www.cs.cmu.edu/~junyanz/" target="_blank">Professor Jun-yan Zhu</a>, and assisted by TAs <a class="underline" href="https://peterwang512.github.io/" target="_blank">Sheng-Yu Wang</a> and <a class="underline" href="https://linzhiqiu.github.io/" target="_blank">Zhi-Qiu Lin</a>.
                        </p>
                        <p class="col-xs-12 mr-4">
                            This course introduces machine learning methods for image and video synthesis. The objectives of synthesis research vary from modeling statistical distributions of visual data, through realistic picture-perfect recreations of the world in graphics, and all the way to providing interactive tools for artistic expression. Key machine learning algorithms will be presented, ranging from classical learning methods (e.g., nearest neighbor, PCA, Markov Random Fields) to deep learning models (e.g., ConvNets, deep generative models, such as GANs and VAEs). We will also introduce image and video forensics methods for detecting synthetic content. In this class, students will learn to build practical applications and create new visual effects using their own photos and videos.
                        </p>
                        <h2>Assigment Summary</h2>
                        <hr class="col-xs-12 mr-4">
                        <div class="row mx-1">
                            <div class="col-12">
                                <p class="col-xs-12">
                                    <table class="table">
                                        <thead>
                                            <tr>
                                                <th scope="col"></th>
                                                <th scope="col">Topic</th>
                                                <th scope="col">Abstract</th>
                                                <th scope="col">Reference</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <th scope="row">A1</th>
                                                <td>Colorizing the Prokudin-Gorskii Photo Collection</td>
                                                <td>Implement SSD, pyramid structure, USM, auto crop, contrast methods </td>
                                                <td>
                                                    <a class="underline" href="http://lcweb2.loc.gov/master/pnp/prok/" target="_blank">Dataset</a><br>
                                                    <a class="underline" href="https://en.wikipedia.org/wiki/Unsharp_masking" target="_blank">USM</a><br>
                                                    <a class="underline" href="https://en.wikipedia.org/wiki/Hough_transform" target="_blank">Hough Transform</a><br>
                                                </td>
                                            </tr>
                                            <tr>
                                                <th scope="row">A2</th>
                                                <td>Gradient Domain Fusion</td>
                                                <td>Implement Poisson Blending, Mixed Blending, Color2Gray</td>
                                                <td>
                                                    <a class="underline" href="https://erkaman.github.io/posts/poisson_blending.html" target="_blank">Poisson Blending</a><br>
                                                </td>
                                            </tr>
                                            <tr>
                                                <th scope="row">A3</th>
                                                <td>When Cats meet GANs</td>
                                                <td>Implement DCGAN, CycleGAN</td>
                                                <td>
                                                    <a class="underline" href="https://data-efficient-gans.mit.edu/datasets/" target="_blank">Dataset</a><br>
                                                    <a class="underline" href="https://github.com/junyanz/pytorch-CycleGAN-and-pix2pix" target="_blank">CycleGAN-pix2pix</a><br>
                                                </td>
                                            </tr>
                                            <tr>
                                                <th scope="row">A4</th>
                                                <td>Neural Style Transfer</td>
                                                <td></td>
                                                <td></td>
                                            </tr>
                                            <tr>
                                                <th scope="row">A5</th>
                                                <td>GAN Photo Editing</td>
                                                <td></td>
                                                <td></td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </p>
                            </div>
                        </div>
                        <h2>Copyright</h2>
                        <hr class="col-xs-12 mr-4">
                        <p class="col-xs-12 mr-4">
                            <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a> All datasets, teaching resources and training networks on this page are copyright by Carnegie Mellon University and published under the <a class="underline" rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license.
                        </p>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade" id="a1" role="tabpanel" aria-labelledby="a1-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #1</h2>
                        <hr class="col-xs-12 mr-4">
                        <h3>Introduction</h3>
                        <p class="col-xs-12 mr-4">
                            The Prokudin-Gorskii image collection from the Library of Congress is a series of glass plate negative photographs taken by Sergei Mikhailovich Prokudin-Gorskii. To view these photographs in color digitally, one must overlay the three images and display them in their respective RGB channels. However, due to the technology used to take these images, the three photos are not perfectly aligned. The goal of this project is to automatically align, clean up, and display a single color photograph from a glass plate negative.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Direct Method</h3>
                        <p class="col-xs-12 mr-4">
                            Before diving into algorithms, I decide to blend R, G, B images directly and have an intuitive feeling of the tasks, which at the same time provides a reference to compare how far my algorithm can go. There are total 1 small jpeg and 9 large tiff images from the given dataset. Their direct blending goes as below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/direct/cathedral.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/emir.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/harvesters.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/icon.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/lady.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/self_portrait.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/three_generations.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/train.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/turkmen.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/village.jpg" height="180px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>SSD & NCC Alignment</h3>
                        <p class="col-xs-12 mr-4">
                            I implement both SSD and NCC to compare the small patch's similarity for alignment, the search range is [-15,15], and the algorithm works both well on the small jpeg image as shown below. To speed up the calculation, I also cropped 20% of each side of the image to decrease calculation on the edge. However, to deal with larger image, not only the search range is not large enough, but also the calulation takes extremely long. Therefore, the pyramid structure comes to practice.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/pyramid/cathedral.jpg" height="280px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>Pyramid Aligment</h3>
                        <p class="col-xs-12 mr-4">
                            I use log2(min(image.shape)) to find out how many mamixmum layers the image can have, and add conditions to only apply the pyramid algorithm for images larger than 512*512, and the small image can directly use [-15,15] SSD search. For large images, my starting layer is a size around 265 pixels(2^8 as first layer), and exhaustively search till the original image size, because I realized missing final layer will give me color bias all the time(misalignment of color channel is very easy to detect even though it's just small pixels). The first implementation of my method took 180s for one image. To speed it up, I recursively decrease search region by 2 each time to shorten it to 55 seconds per iamge, because the center of search box is determined by last layer, so the deeper algorithm search, the smaller search range it requires to find the best alignment. The output is listed as below. As you can see, most of the images are aligned quite well but image like the piece of Sergei Mikhailovich Prokudin-Gorskii, work even worse than direct alignment, this is because the brightness of the images are different, therefore, I use an USM(Unsharp Mask) algorithm to fix the issue.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/pyramid/cathedral.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/emir.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/harvesters.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/icon.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/lady.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/self_portrait.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/three_generations.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/train.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/turkmen.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/village.jpg" height="180px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h2>Extra Credits</h2>
                        <h3>USM Unsharp Mask</h3>
                        <p class="col-xs-12 mr-4">
                            The USM algorithm is mainly to sharpen or soften the edge of images, and allows accurate SSD difference to make better alignment. The algorithm is called for each recursion in the pyramid alignment, and it first uses Gaussian Blur to blur the single channel(gray) image, and subtract it from the original image, then I take the region of difference that is larger than certain threshold and subtract them from the original image and multiple certain constant parameters. Here I use subraction because I notice certain edge needs to be softened instead of being sharpened, due to some disturbing edges stand out too much in the original image that makes SSD find the wrong alignment. The USM specifically improves the quality of this image:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/usm/emir.jpg" height="280px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>Crop Image</h3>
                        <p class="col-xs-12 mr-4">
                            To get rid of the borders, I mainly use two cropping method, the first one is to keep area only for all three channels have contents, and remove those blank area caused by alignment, this is implemented by retriving shift of each single channel image. But this method can't deal so well with the region that is originally black or white outside the image. Then I use a MSE(Mean Square Error) method to calculate each row and each column's error, and set up when three adjacent rows or columns all are smaller than certain threshold, it is the area should be cropped. The result shows as below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/crop/cathedral.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/emir.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/harvesters.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/icon.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/lady.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/self_portrait.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/three_generations.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/train.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/turkmen.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/village.jpg" height="200px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>Add Contrast</h3>
                        <p class="col-xs-12 mr-4">
                            The contrast method is pretty straight-forward, I just calculate the accumulative histogram of the image, and take 5% and 95% as 0 and 255 respectively, and stretch the color value in between so that the contrast of the main image increase.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/contrast/cathedral.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/emir.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/harvesters.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/icon.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/lady.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/self_portrait.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/three_generations.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/train.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/turkmen.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/village.jpg" height="200px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>Other Dataset</h3>
                        <p class="col-xs-12 mr-4">
                            I find some other similar <a class="underline" href="http://lcweb2.loc.gov/master/pnp/prok/" target="_blank">dataset</a> that has pretty large tiff image to test the algorithm. The result shows as below
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/outdata_crop/00001a.jpg" height="220px" alt="image_left">
                                <img src="./media/a1/outdata_crop/00002u.jpg" height="220px" alt="image_left">
                                <img src="./media/a1/outdata_crop/00004a.jpg" height="220px" alt="image_left">
                                <img src="./media/a1/outdata_crop/00005u.jpg" height="220px" alt="image_left">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade" id="a2" role="tabpanel" aria-labelledby="a2-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #2</h2>
                        <hr class="col-xs-12 mr-4">
                        <h3>Introduction</h3>
                        <p class="col-xs-12 mr-4">
                            The project explores the gradient-domain processing in the practice of image blending, tone mapping and non-photorealistic rendering. The method mainly focuses on the Poisson Blending algorithm. The tasks include primary gradient minimization, 4 neighbours based Poisson blending, mixed gradient Poisson blending and grayscale intensity preserved color2gray method. The whole project is implemented in Python.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Toy Problem</h3>
                        <p class="col-xs-12 mr-4">
                            The toy problem is a simplifed version of Poisson blending algorithm, therefore it helps understanding the Poisson blending a lot. The major functions are three, to calculate the gradient of x axis and y axis, and to align the left-top corner (0,0) of the image:
                        </p>
                        <p class="col-xs-12 mr-4">
                            <b>
                            ((v(x+1,y)−v(x,y))−(s(x+1,y)−s(x,y)))**2<br>
                            ((v(x,y+1)−v(x,y))−(s(x,y+1)−s(x,y)))**2<br>
                             (v(1,1)−s(1,1))**2
                            </b>
                        </p>
                        <p class="col-xs-12 mr-4">
                            We use the equations to loop through each pixel of the source image to construct A and b. By solving the least square form of Av=b, we can get the synthesized image v. Say the given gray image size is H by W, A's dimension is <b>H*W</b> by <b>2*H*W+1</b>, b's dimension is <b>H*W</b> by <b>1</b>. The result is pretty much to test if we can copy the original image, as shown below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/toy.png" height="400px" alt="image_left">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            I implemented it in both loop method and non-loop method, the interesting thing is that with loop method, it only takes around 0.4s, whereas with the non-loop method, which is supposed to be faster in Python environment than loop, turns out to take around 10s. I think the major reason is that sparse matrix's arithmetic calculation is more expensive than directly assign value to coordinates. In the non-loop method, I mainly use lil_matrix to construct sparse matrix, and use np.roll, np.transpose to construct A matrix. Finally I decide to use the loop method for the rest of the task.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Poisson Blending</h3>
                        <p class="col-xs-12 mr-4">
                            Based on toy problem's hint, Poisson Blending explores the four neighbour of each pixel, follow the equation below that <b>v</b> is the synthesized vector that we need to solve, <b>s</b> is the source image in the size of target image(but we're only interested in the masked source image), <b>t</b> is the target image:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/poisson.png" height="80px" alt="image_left">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            In the equation, each <b>i</b> deals with 4 <b>j</b>, i.e. the same <b>i</b> is calculated 4 times with 4 different neighbour <b>j</b>. The left part considers the condition if all the neighbour of i is still inside the mask, and the right part considers the neighbour not all inside the mask. Notice the difference in code is that for the right part, <b>tj</b> is used to construct parameter <b>b</b>, whereas for the left part, <b>vj</b> is used to construct parameter <b>A</b>.
                        </p>
                        <p class="col-xs-12 mr-4">
                            Also, the given image now has RGB, 3 channels, therefore we need to calculate each channel separately. When I implemented it, I consider that <b>A</b> matrix is always the same for 3 channels, but <b>b</b> are different for each channel, therefore I only calculate <b>A</b> once and <b>b</b> 3 times to speed up the algorithm. What's more, since the we only need to generate image from the maksed source image's coordinate, I only loop through this area to speed up. And finally I merged three v together to get new RGB image. The average speed is related to the image size, to deal with the given example of <b>130x107</b>(source image) and <b>250x333</b>(taget image), it generally takes around 20s.
                        </p>
                        <p class="col-xs-12 mr-4">
                            However, the naive Poisson Blending has some issue to deal with blending the image seamlessly, which is because only considering the source neighbour is not enough for strong difference of target and source images. As you can see the image below, the inner part of the ballon house is little blur and not blended so well with the background. Therefore, we need to implement Mixed Blending.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/house.jpg" height="150px">
                                <img src="./media/a2/house_mask.png" height="150px">
                                <img src="./media/a2/mountain.jpg" height="350px">
                                <img src="./media/a2/ballon_blend.png" height="600px">
                            </div>
                        </div>
                        <h2>Extra Credits</h2>
                        <h3>Mixed Gradients</h3>
                        <p class="col-xs-12 mr-4">
                            Mixed gradients is actually pretty straight forward based on Poisson Blending, we just add one condition that instead calculating the gradient of source image <b>s</b>, we compare the gradient of <b>s</b> and gradient of <b>t</b>, and take the larger one so that it can better blend when the difference of source and target image are large. The comparison of the same image can be seen as below(The left is blendered without Mixed gradients.):
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/ballon_blend.png" height="500px">
                                <img src="./media/a2/ballon.png" height="500px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            The given example of bear and pool is below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/source_01.jpg" height="150px">
                                <img src="./media/a2/source_01_mask.png" height="150px">
                                <img src="./media/a2/target_01.jpg" height="350px">
                                <img src="./media/a2/poisson_blend.png" height="600px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            There are some more generated blending images:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/obama.jpg" height="150px">
                                <img src="./media/a2/obama_mask.png" height="150px">
                                <img src="./media/a2/mona_lisa.jpg" height="300px">
                                <img src="./media/a2/obama.png" height="600px">
                            </div>
                        </div>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/swimmer.jpg" height="100px">
                                <img src="./media/a2/swimmer_mask.png" height="100px">
                                <img src="./media/a2/road.jpg" height="250px">
                                <img src="./media/a2/swimmer.png" height="600px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            And there is a failure case where the colorful Patrix can't blend so well with the gray-scale like moon image:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/patrick.jpg" height="100px">
                                <img src="./media/a2/patrick_mask.png" height="100px">
                                <img src="./media/a2/moon.jpg" height="200px">
                                <img src="./media/a2/patrick.png" height="600px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            This is mainly because there is upper limitation of the blending algorithm to adjust the color, if the source image and target image have too large difference, the algorithm will reach its limitation to find the solution that best approximate the least square function.
                        </p>
                        <h3>Color2Gray</h3>
                        <p class="col-xs-12 mr-4">
                            The Color2Gray method first turns RGB image to the HSV color space, and only consider the S and V channels to represent the color contrast and intensity respectively. In this way, we can keep the color contrast of rgb image and preserve the grayscale intensity at the same time. The algorithm runs similar to the Mixed Gradient where source and target image are S and V. The result is shown as below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/colorBlindTest.png" height="300px">
                                <img src="./media/a2/color2gray.png" height="600px">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade show active" id="a3" role="tabpanel" aria-labelledby="a3-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #3</h2>
                        <hr class="col-xs-12 mr-4">
                        <h3>Introduction</h3>
                        <p class="col-xs-12 mr-4">
                            This project implements two famous GAN architecture: DCGAN and CycleGAN. It is programmed in Pytorch, the major code includes the build-up of discriminator and generator neural network, loss function, forward and backward propagations. It also explores different methods that help GAN generate better results, such as Data Agumentation, Differentiable Augmentation, variance of different lose functions, variance of different discriminators, and implemented in different dataset to check the robustness fo the network.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Part I: Deep Convulotional GAN</h3>

                        <p class="col-xs-12 mr-4"><b>Implement Data Augmentation</b></p>
                        <p class="col-xs-12 mr-4">
                            In Pytorch, data augmentation is invoked each time when iterated through the mini-batch, the purpose of the data augmentation is add variance to the dataset, which is especially useful when the dataset is small or each sample of the data are too similar. For example, in this DCGAN training, we only have 204 images as dataset. I mainly use the Resize + RandomCrop, RandomHorizontalFlip, RandomRotation(10 angles) to generate augmentation. The example of orignal dataset and augmented dataset, and GAN generated sample by different dataset can be see here:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/real-basic.png" height="300px">
                                <img src="./media/a3/real-basic_diff.png" height="300px">
                                <img src="./media/a3/real-deluxe.png" height="300px">
                                <img src="./media/a3/real-deluxe-diff.png" height="300px">
                            </div>
                        </div>
                        
                        <p class="col-xs-12 mr-4"><b>The Discriminator</b></p>
                        <p class="col-xs-12 mr-4">
                            The discriminator in this DCGAN is a convolutional neural network with the following architecture:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/discriminator.png" height="300px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            According to the formula <b>(N-F+2P)/S + 1 = M</b>(We don't consider dilation here), where <b>N</b> is the input number of channels, <b>M</b> is the output number of channels, <b>S</b> is stride, <b>P</b> is padding, <b>F</b> is size of filter/kernel, since we want <b>N=2M</b>, and we use filter size of 4, stride of 2, we can calculate that padding <b>P</b> is 1.
                            Besides, I use softmax for the output layer, and later on squared mean difference for loss function, since this Discriminator is a classification problem, the softmax-loss combination is a generally good choice.
                        </p>

                        <p class="col-xs-12 mr-4"><b>The Generator</b></p>
                        <p class="col-xs-12 mr-4">
                            The generator in this DCGAN is a convolutional neural network with the following architecture:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/generator.png" height="450px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            In the generator neural network, I use transposed convolution with a filter, size of 4, stride of 1 and padding of 0 for the very first layer that from 100x1x1 input to 256x4x4 output. And the rest layer all are upsampling of 2, with a filter, size of 3, stride of 1 and padding of 1 to satisfy the condition that output dimension is the 2 times of input dimension. And the reason why the first layer is a transposed convolution is that it has better performance than direct upsampling for noise. 
                        </p>

                        <p class="col-xs-12 mr-4"><b>The Training Loop</b></p>
                        <p class="col-xs-12 mr-4">
                            The training loop including loss function and backpropagation is as straight forward as image below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/gan_algo.png" height="450px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            One thing to notice is that we normally first do the back propagation to update the gradients and weights of discriminator, then do the generator, this is because the loss function first goes back to discriminator then to the generator. To better train generator's weight, we prefer the gradient and weights of discriminator is static than dynamic. 
                        </p>
                        <p class="col-xs-12 mr-4">
                            Similarly, when training the discriminator, we don't want to update the gradient all the way back to generator, therefore we have to set no_grad_up for the fake_image that is generated from generator and fed to discriminator. In my case, I use torch.detach() function.
                        </p>

                        <p class="col-xs-12 mr-4"><b>The Differentiable Augmentation</b></p>
                        <p class="col-xs-12 mr-4">
                            The Differentiable Augmentation method is meant to process data during the training process, it can slow down the training but can significantly improve the output's performance. And it's shown as below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/method.jpg" height="100px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            I apply differentiable augmentation to all the generator generated fake images and real images during the training process.
                        </p>

                        <p class="col-xs-12 mr-4"><b>Results</b></p>
                        <p class="col-xs-12 mr-4">
                            After the training with learning rate = 0.0002, beta1 = 0.5, beta2 = 0.999, epoch = 500, batch_size = 16, I get the results as below. From left to right, they are generated image with basic data augmentation, with basic and differentiable data augmentation, with deluxe data augmentation, and with deluxe and differentiable data augmentation.
                        </p>

                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/basic.png" height="300px">
                                <img src="./media/a3/basic_diff.png" height="300px">
                                <img src="./media/a3/deluxe.png" height="300px">
                                <img src="./media/a3/deluxe_diff.png" height="300px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            The result is interesting. It's obvious the basic method has a clumzy, hard to tell cat generation. And the differentiable method clearly has some color intensity and white balance shift, it actually proves the ability of this method to increase the robustness towards the RGB color intensity, and generate a more general color tone results, also it has a better recognition in the shape of cat. The deluxe result has a better detail than the differential method. And finally, the mix of deluxe and differential method is hard to tell if it's better than a single method, because some of the generation has both good detail and color tone but some are more blur or distorted, but if we only want one best result from a group of samples, this mix method definitely works better than either of the single method.
                        </p>
                        <p class="col-xs-12 mr-4">
                            Similarly to the order above, I also draw the loss curve along with every 200 iteration as below:
                        </p>

                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/loss_basic.png" height="250px">
                                <img src="./media/a3/loss_basic_diff.png" height="250px">
                                <img src="./media/a3/loss_deluxe.png" height="250px">
                                <img src="./media/a3/loss_deluxe_diff.png" height="250px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            From the loss curve, we can tell that the base and deluxe both have the trend to overfitting after the training, though dexule has a relatively better loss(where G and D both are close to 0.5), and with the help of differentiable augmentation, the loss is more close to the ideal value, which means the network has a better robust performance on general data.
                        </p>
                        <hr class="col-xs-12 mr-4">

                        <h3>Part II: CycleGAN</h3>
                        <p class="col-xs-12 mr-4"><b>The Dual Generator</b></p>
                        <p class="col-xs-12 mr-4">
                            The generator of CycleGAN is a convolutional neural network with the following architecture:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/cyclegan_generator.png" height="450px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            In the generator neural network, there are two generator for X->Y and Y->X, they are symmetric in input and output, and their architecture are identical in this project. The filter size is the same as in DCGAN, refer to them in convolution and upconvolution layer respectively. And one major difference is the ResnetBlock, which is used 3 times here, which aims to make sure characteristics of the output image (e.g., the shapes of objects) do not differ too much from the input. One major difference of this generator from the one of DCGAN is that its input is not noise anymore but an image.
                        </p>

                        <p class="col-xs-12 mr-4"><b>The PatchDiscriminator</b></p>
                        <p class="col-xs-12 mr-4">
                            A major difference of this PatchDiscriminator from the Discriminator of DCGAN is that its output is a 4x4 patch instead of a loss value, which means the output layer doesn't require a softmax function anymore, and the rest are pretty much the same.
                        </p>

                        <p class="col-xs-12 mr-4"><b>The Training Loop</b></p>
                        <p class="col-xs-12 mr-4">
                            The training loop including loss function and backpropagation is as straight forward as image below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/cyclegan_algo.png" height="550px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            The major loss function and pipeline are similar to DCGAN, except for that for generator, since we have two generators, and we want to train them at the same time, so we add both X->Y and Y->X loss here; And for the discriminator, it goes the same way to train the loss from fake images generated from X->Y and Y->X.
                        </p>

                        <p class="col-xs-12 mr-4"><b>The Cycle Consistency</b></p>
                        <p class="col-xs-12 mr-4">
                            The cycle consistency is like the soul part of the CycleGAN from my own experience of experiment, it aims to 
                        </p>

                        <p class="col-xs-12 mr-4"><b>The CycleGAN Experiments</b></p>
                        <p class="col-xs-12 mr-4">
                            My training for CycleGAN follows the learning rate = 0.0002, beta1 = 0.5, beta2 = 0.999, batch_size = 16 same as DCGAN, and I always use the Differentiable Augmentation since it helps increase robustness towards the RGB intensity and other factors. I first start by testing the CycleGAN with epoch = 1000, with and without cycle consistency, and the results are(Left side are without cycle, right side are with cycle; up side are X->Y, down side are Y->X): 
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/sample-001000-X-Y.png" height="350px">
                                <img src="./media/a3/sample-cycle-001000-X-Y.png" height="350px">
                            </div>
                        </div>
                        <br>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/sample-001000-Y-X.png" height="350px">
                                <img src="./media/a3/sample-cycle-001000-Y-X.png" height="350px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            It's clearly that 1000 epoch is not enough to get a good output, but from the basic shape of it, and the loss function below, we can tell that the general direction is good to extend the epoch.(Left without cycle, right with cyckle) And we can tell that the cycle consistency has a slightly better output.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/loss_10000_cat_nocycle.png" height="350px">
                                <img src="./media/a3/loss_10000_cat_allon.png" height="350px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            Next I extend the epoch to 10000, and train the CycleGAN with two datasets, also I made comparison that with and without cycle consistency, patchDiscriminator and DCDiscriminator. The result are listed below:(Left is patch+cycle, middle is patch no cycle, right is dc+cycle)
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/cycle-sample-010000-X-Y.png" height="200px">
                                <img src="./media/a3/nocycle-sample-010000-X-Y.png" height="200px">
                                <img src="./media/a3/dc-cycle-sample-010000-X-Y.png" height="200px">
                            </div>
                        </div>
                        <br>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/cycle-sample-010000-Y-X.png" height="200px">
                                <img src="./media/a3/nocycle-sample-010000-Y-X.png" height="200px">
                                <img src="./media/a3/dc-cycle-sample-010000-Y-X.png" height="200px">
                            </div>
                        </div>
                        <br>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/fruit-cycle-sample-010000-X-Y.png" height="200px">
                                <img src="./media/a3/fruit-nocycle-sample-010000-X-Y.png" height="200px">
                                <img src="./media/a3/fruit-dc-sample-010000-X-Y.png" height="200px">
                            </div>
                        </div>
                        <br>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/fruit-cycle-sample-010000-Y-X.png" height="200px">
                                <img src="./media/a3/fruit-nocycle-sample-010000-Y-X.png" height="200px">
                                <img src="./media/a3/fruit-dc-sample-010000-Y-X.png" height="200px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            By observing the results above, we can see that without cycle consistency(middle column), the generated results have weird color, unclear shape. And for the DCDiscriminator(right column) and PatchDiscriminator(left column), they both achieve a relatively good result that generated fake image assembles the real image a lot. And they both have this color re-mapping effect, by comparison, I think the PatchDiscriminator has a slightly stronger color shift in all interested regions. It means that PatchDiscriminator can perform better pattern rematch effect.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Bonus</h3>
                        <p class="col-xs-12 mr-4"><b>Extra dataset</b></p>
                        <p class="col-xs-12 mr-4">
                            I choose one dataset from <a href="https://data-efficient-gans.mit.edu/datasets/">https://data-efficient-gans.mit.edu/datasets/</a> to apply to DCGAN, the output is like:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/obama-dc.png" height="400px">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade" id="a4" role="tabpanel" aria-labelledby="a4-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #4</h2>
                        <hr class="col-xs-12 mr-4">
                        <p class="col-xs-12 mr-4">Lorem ipsum dolor sit amet, no eum reque putent scripta. Eos esse vidit nonumy eu. Sit ut nominati prodesset, affert consul expetendis et cum. Sea cu animal cotidieque. Ei qui affert voluptua hendrerit, eripuit menandri pericula et per. Eam cu iusto possim vocent, veri mazim sensibus ea has, salutandi periculis his eu. Mei tractatos sententiae no.
                            Eu vim eirmod molestie electram, eam ad dicunt facilisi conclusionemque, at nostrud delectus incorrupte mea. Vel ea liber munere maluisset, cu habeo cotidieque vel, blandit indoctum ei vel. Sit invidunt erroribus ne. Ea postea possit persecuti qui, tota dicit discere ut usu. Mei te iudicabit repudiare. Legere tritani definitiones vix ei, mea ea eros singulis.
                            Qui ne posidonium definitionem, maluisset repudiare appellantur eam et, vim et vivendum facilisi delicatissimi. Ius ei choro dictas. Sit in atqui antiopam, ut his iuvaret oportere sapientem. Solet equidem recteque eum at, nam mundi perpetua te.
                            Eum te semper appareat omittantur. Labitur perpetua ea mea, nam ea impetus partiendo. Elitr perfecto adipisci ut eum, his vocibus tincidunt incorrupte et. Ad duo putant tractatos. Sint case qualisque vis cu, soluta percipitur eu eam, cum inermis definitionem cu. Ne summo democritum pri, et falli ludus eruditi nam.
                            Per assum mucius ex, no nec petentium delicatissimi, et pri hinc tacimates similique. Purto prima omnes vel ad, eu feugiat nostrum eum, ut has idque laoreet periculis. Ad vel officiis lucilius accusata, nonumy volutpat te qui. Noster indoctum mediocritatem ne cum.</p>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade" id="a5" role="tabpanel" aria-labelledby="a5-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #5</h2>
                        <hr class="col-xs-12 mr-4">
                        <p class="col-xs-12 mr-4">Lorem ipsum dolor sit amet, no eum reque putent scripta. Eos esse vidit nonumy eu. Sit ut nominati prodesset, affert consul expetendis et cum. Sea cu animal cotidieque. Ei qui affert voluptua hendrerit, eripuit menandri pericula et per. Eam cu iusto possim vocent, veri mazim sensibus ea has, salutandi periculis his eu. Mei tractatos sententiae no.
                            Eu vim eirmod molestie electram, eam ad dicunt facilisi conclusionemque, at nostrud delectus incorrupte mea. Vel ea liber munere maluisset, cu habeo cotidieque vel, blandit indoctum ei vel. Sit invidunt erroribus ne. Ea postea possit persecuti qui, tota dicit discere ut usu. Mei te iudicabit repudiare. Legere tritani definitiones vix ei, mea ea eros singulis.
                            Qui ne posidonium definitionem, maluisset repudiare appellantur eam et, vim et vivendum facilisi delicatissimi. Ius ei choro dictas. Sit in atqui antiopam, ut his iuvaret oportere sapientem. Solet equidem recteque eum at, nam mundi perpetua te.
                            Eum te semper appareat omittantur. Labitur perpetua ea mea, nam ea impetus partiendo. Elitr perfecto adipisci ut eum, his vocibus tincidunt incorrupte et. Ad duo putant tractatos. Sint case qualisque vis cu, soluta percipitur eu eam, cum inermis definitionem cu. Ne summo democritum pri, et falli ludus eruditi nam.
                            Per assum mucius ex, no nec petentium delicatissimi, et pri hinc tacimates similique. Purto prima omnes vel ad, eu feugiat nostrum eum, ut has idque laoreet periculis. Ad vel officiis lucilius accusata, nonumy volutpat te qui. Noster indoctum mediocritatem ne cum.</p>
                    </div>
                </div>
            </div>
        </div>
        <!-- footer -->
        <div id="footer" class="text-center py-3">©2022 Copyright | <a href="http://caoyuchen.github.io" target="_blank">Joshua Cao</a> | yuchenca@andrew.cmu.edu</div>
    </div>
</body>

</html>
<!-- cloud effect -->
<script>
VANTA.CLOUDS2({
    el: "#cloud",
    mouseControls: true,
    touchControls: true,
    gyroControls: false,
    minHeight: 200.00,
    minWidth: 200.00,
    scale: 1.00,
    // skyColor: 0x6381b1,
    // cloudColor: 0x646493,
    speed: 1.50,
    texturePath: "./media/noise.png"
})
</script>
