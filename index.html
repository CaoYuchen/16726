<!--
Dependecies:
jQuery JavaScript Library v1.4.2
http://jquery.com/
Copyright 2010, John Resig
Dual licensed under the MIT or GPL Version 2 licenses.
http://jquery.org/license
-----------------------------------------------------
Bootstrap
Copyright (c) 2011-2019 Twitter, Inc.
Copyright (c) 2011-2019 The Bootstrap Authors
https://github.com/twbs/bootstrap/blob/v4.3.1/LICENSE
-----------------------------------------------------
Vanta
https://github.com/tengbao/vanta
-----------------------------------------------------
Author: Joshua Cao | yuchenca@andrew.cmu.edu
Webiste: 16726 course website
-->
<!doctype html>
<html>

<head>
    <!-- icon -->
    <link rel="icon" href="./media/joshua.ico" type="image/x-icon">
    <link rel="shortcut icon" href="./media/joshua.ico" type="image/x-icon">
    <link rel="bookmark" href="./media/joshua.ico" type="image/x-icon">
    <!-- title -->
    <title>16726-Yuchenca</title>
    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge,chrome=1">
    <meta content="width=device-width, initial-scale=1.0, maximum-scale=1.0, user-scalable=0" name="viewport">
    <meta name="description" content="16726-Yuchenca">
    <meta name="keywords" lang="de" content="16726-Yuchenca">
    <!-- jQuery -->
    <script type="text/javascript" src="./js/jQuery-3.3.1.js"></script>
    <!-- Bootstrap -->
    <link type="text/css" href="./css/bootstrap.min.css" rel='stylesheet'>
    <script type="text/javascript" src="./js/bootstrap.min.js"></script>
    <!-- main -->
    <link type="text/css" href="./css/main.css" rel='stylesheet'>
    <script type="text/javascript" src="./js/main.js"></script>
    <!-- vanta -->
    <script type="text/javascript" src="./js/three.min.js"></script>
    <script type="text/javascript" src="./js/ring.min.js"></script>
    <script type="text/javascript" src="./js/cloud.min.js"></script>
</head>

<body>
    <div style="width:100%;height:100%">
        <!-- title -->
        <div  id="cloud">
            <div class="row">
                <div class="col-12 mx-12">
                    <div id="title" class="text-center">
                        <span id="title-top" class="font-weight-bold">16726 Learning-based Image Synthesis Spring 22</span>
                        <br>
                        <span id="title-bot">Joshua Cao | Carnegie Mellon University</span>
                    </div>
                </div>
            </div>
            <br><br>
            <!-- navigation -->
            <ul id="navigation" class="nav nav-tabs font-weight-bold text-center" role="tablist">
                <li class="nav-item">
                    <a class="nav-link" id="home-tab" data-toggle="tab" href="#home" role="tab" aria-controls="home" aria-selected="false">Home</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="a1-tab" data-toggle="tab" href="#a1" role="tab" aria-controls="a1" aria-selected="false">Assignment1</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="a2-tab" data-toggle="tab" href="#a2" role="tab" aria-controls="a2" aria-selected="false">Assignment2</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link active" id="a3-tab" data-toggle="tab" href="#a3" role="tab" aria-controls="a3" aria-selected="true">Assignment3</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="a4-tab" data-toggle="tab" href="#a4" role="tab" aria-controls="a4" aria-selected="false">Assignment4</a>
                </li>
                <li class="nav-item">
                    <a class="nav-link" id="a5-tab" data-toggle="tab" href="#a5" role="tab" aria-controls="a5" aria-selected="false">Assignment5</a>
                </li>
            </ul>
        </div>
        <div class="tab-content container" id="TabContent">
            <div class="tab-pane fade" id="home" role="tabpanel" aria-labelledby="home-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>About Course</h2>
                        <hr class="col-xs-12 mr-4">
                        <p class="col-xs-12 mr-4">
                            <a class="underline" href="https://learning-image-synthesis.github.io/sp22/" target="_blank">16-726 Learning-Based Image Synthesis / Spring 2022</a> is led by <a class="underline" href="https://www.cs.cmu.edu/~junyanz/" target="_blank">Professor Jun-yan Zhu</a>, and assisted by TAs <a class="underline" href="https://peterwang512.github.io/" target="_blank">Sheng-Yu Wang</a> and <a class="underline" href="https://linzhiqiu.github.io/" target="_blank">Zhi-Qiu Lin</a>.
                        </p>
                        <p class="col-xs-12 mr-4">
                            This course introduces machine learning methods for image and video synthesis. The objectives of synthesis research vary from modeling statistical distributions of visual data, through realistic picture-perfect recreations of the world in graphics, and all the way to providing interactive tools for artistic expression. Key machine learning algorithms will be presented, ranging from classical learning methods (e.g., nearest neighbor, PCA, Markov Random Fields) to deep learning models (e.g., ConvNets, deep generative models, such as GANs and VAEs). We will also introduce image and video forensics methods for detecting synthetic content. In this class, students will learn to build practical applications and create new visual effects using their own photos and videos.
                        </p>
                        <h2>Assigment Summary</h2>
                        <hr class="col-xs-12 mr-4">
                        <div class="row mx-1">
                            <div class="col-12">
                                <p class="col-xs-12">
                                    <table class="table">
                                        <thead>
                                            <tr>
                                                <th scope="col"></th>
                                                <th scope="col">Topic</th>
                                                <th scope="col">Abstract</th>
                                                <th scope="col">Reference</th>
                                            </tr>
                                        </thead>
                                        <tbody>
                                            <tr>
                                                <th scope="row">A1</th>
                                                <td>Colorizing the Prokudin-Gorskii Photo Collection</td>
                                                <td>Implement SSD, pyramid structure, USM, auto crop, contrast methods </td>
                                                <td>
                                                    <a class="underline" href="http://lcweb2.loc.gov/master/pnp/prok/" target="_blank">Dataset</a><br>
                                                    <a class="underline" href="https://en.wikipedia.org/wiki/Unsharp_masking" target="_blank">USM</a><br>
                                                    <a class="underline" href="https://en.wikipedia.org/wiki/Hough_transform" target="_blank">Hough Transform</a><br>
                                                </td>
                                            </tr>
                                            <tr>
                                                <th scope="row">A2</th>
                                                <td>Gradient Domain Fusion</td>
                                                <td></td>
                                                <td></td>
                                            </tr>
                                            <tr>
                                                <th scope="row">A3</th>
                                                <td>When Cats meet GANs</td>
                                                <td></td>
                                                <td></td>
                                            </tr>
                                            <tr>
                                                <th scope="row">A4</th>
                                                <td>Neural Style Transfer</td>
                                                <td></td>
                                                <td></td>
                                            </tr>
                                            <tr>
                                                <th scope="row">A5</th>
                                                <td>GAN Photo Editing</td>
                                                <td></td>
                                                <td></td>
                                            </tr>
                                        </tbody>
                                    </table>
                                </p>
                            </div>
                        </div>
                        <h2>Copyright</h2>
                        <hr class="col-xs-12 mr-4">
                        <p class="col-xs-12 mr-4">
                            <a rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-nc-sa/4.0/88x31.png"></a> All datasets, teaching resources and training networks on this page are copyright by Carnegie Mellon University and published under the <a class="underline" rel="license" href="http://creativecommons.org/licenses/by-nc-sa/4.0/">Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License</a>. This means that you must attribute the work in the manner specified by the authors, you may not use this work for commercial purposes and if you alter, transform, or build upon this work, you may distribute the resulting work only under the same license.
                        </p>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade" id="a1" role="tabpanel" aria-labelledby="a1-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #1</h2>
                        <hr class="col-xs-12 mr-4">
                        <h3>Introduction</h3>
                        <p class="col-xs-12 mr-4">
                            The Prokudin-Gorskii image collection from the Library of Congress is a series of glass plate negative photographs taken by Sergei Mikhailovich Prokudin-Gorskii. To view these photographs in color digitally, one must overlay the three images and display them in their respective RGB channels. However, due to the technology used to take these images, the three photos are not perfectly aligned. The goal of this project is to automatically align, clean up, and display a single color photograph from a glass plate negative.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Direct Method</h3>
                        <p class="col-xs-12 mr-4">
                            Before diving into algorithms, I decide to blend R, G, B images directly and have an intuitive feeling of the tasks, which at the same time provides a reference to compare how far my algorithm can go. There are total 1 small jpeg and 9 large tiff images from the given dataset. Their direct blending goes as below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/direct/cathedral.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/emir.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/harvesters.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/icon.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/lady.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/self_portrait.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/three_generations.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/train.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/turkmen.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/direct/village.jpg" height="180px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>SSD & NCC Alignment</h3>
                        <p class="col-xs-12 mr-4">
                            I implement both SSD and NCC to compare the small patch's similarity for alignment, the search range is [-15,15], and the algorithm works both well on the small jpeg image as shown below. To speed up the calculation, I also cropped 20% of each side of the image to decrease calculation on the edge. However, to deal with larger image, not only the search range is not large enough, but also the calulation takes extremely long. Therefore, the pyramid structure comes to practice.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/pyramid/cathedral.jpg" height="280px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>Pyramid Aligment</h3>
                        <p class="col-xs-12 mr-4">
                            I use log2(min(image.shape)) to find out how many mamixmum layers the image can have, and add conditions to only apply the pyramid algorithm for images larger than 512*512, and the small image can directly use [-15,15] SSD search. For large images, my starting layer is a size around 265 pixels(2^8 as first layer), and exhaustively search till the original image size, because I realized missing final layer will give me color bias all the time(misalignment of color channel is very easy to detect even though it's just small pixels). The first implementation of my method took 180s for one image. To speed it up, I recursively decrease search region by 2 each time to shorten it to 55 seconds per iamge, because the center of search box is determined by last layer, so the deeper algorithm search, the smaller search range it requires to find the best alignment. The output is listed as below. As you can see, most of the images are aligned quite well but image like the piece of Sergei Mikhailovich Prokudin-Gorskii, work even worse than direct alignment, this is because the brightness of the images are different, therefore, I use an USM(Unsharp Mask) algorithm to fix the issue.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/pyramid/cathedral.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/emir.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/harvesters.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/icon.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/lady.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/self_portrait.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/three_generations.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/train.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/turkmen.jpg" height="180px" alt="image_left">
                                <img src="./media/a1/pyramid/village.jpg" height="180px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h2>Extra Credits</h2>
                        <h3>USM Unsharp Mask</h3>
                        <p class="col-xs-12 mr-4">
                            The USM algorithm is mainly to sharpen or soften the edge of images, and allows accurate SSD difference to make better alignment. The algorithm is called for each recursion in the pyramid alignment, and it first uses Gaussian Blur to blur the single channel(gray) image, and subtract it from the original image, then I take the region of difference that is larger than certain threshold and subtract them from the original image and multiple certain constant parameters. Here I use subraction because I notice certain edge needs to be softened instead of being sharpened, due to some disturbing edges stand out too much in the original image that makes SSD find the wrong alignment. The USM specifically improves the quality of this image:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/usm/emir.jpg" height="280px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>Crop Image</h3>
                        <p class="col-xs-12 mr-4">
                            To get rid of the borders, I mainly use two cropping method, the first one is to keep area only for all three channels have contents, and remove those blank area caused by alignment, this is implemented by retriving shift of each single channel image. But this method can't deal so well with the region that is originally black or white outside the image. Then I use a MSE(Mean Square Error) method to calculate each row and each column's error, and set up when three adjacent rows or columns all are smaller than certain threshold, it is the area should be cropped. The result shows as below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/crop/cathedral.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/emir.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/harvesters.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/icon.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/lady.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/self_portrait.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/three_generations.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/train.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/turkmen.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/crop/village.jpg" height="200px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>Add Contrast</h3>
                        <p class="col-xs-12 mr-4">
                            The contrast method is pretty straight-forward, I just calculate the accumulative histogram of the image, and take 5% and 95% as 0 and 255 respectively, and stretch the color value in between so that the contrast of the main image increase.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/contrast/cathedral.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/emir.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/harvesters.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/icon.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/lady.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/self_portrait.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/three_generations.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/train.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/turkmen.jpg" height="200px" alt="image_left">
                                <img src="./media/a1/contrast/village.jpg" height="200px" alt="image_left">
                            </div>
                        </div>
                        <hr class="col-xs-12 mr-4">
                        <h3>Other Dataset</h3>
                        <p class="col-xs-12 mr-4">
                            I find some other similar <a class="underline" href="http://lcweb2.loc.gov/master/pnp/prok/" target="_blank">dataset</a> that has pretty large tiff image to test the algorithm. The result shows as below
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a1/outdata_crop/00001a.jpg" height="220px" alt="image_left">
                                <img src="./media/a1/outdata_crop/00002u.jpg" height="220px" alt="image_left">
                                <img src="./media/a1/outdata_crop/00004a.jpg" height="220px" alt="image_left">
                                <img src="./media/a1/outdata_crop/00005u.jpg" height="220px" alt="image_left">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade" id="a2" role="tabpanel" aria-labelledby="a2-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #2</h2>
                        <hr class="col-xs-12 mr-4">
                        <h3>Introduction</h3>
                        <p class="col-xs-12 mr-4">
                            The project explores the gradient-domain processing in the practice of image blending, tone mapping and non-photorealistic rendering. The method mainly focuses on the Poisson Blending algorithm. The tasks include primary gradient minimization, 4 neighbours based Poisson blending, mixed gradient Poisson blending and grayscale intensity preserved color2gray method. The whole project is implemented in Python.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Toy Problem</h3>
                        <p class="col-xs-12 mr-4">
                            The toy problem is a simplifed version of Poisson blending algorithm, therefore it helps understanding the Poisson blending a lot. The major functions are three, to calculate the gradient of x axis and y axis, and to align the left-top corner (0,0) of the image:
                        </p>
                        <p class="col-xs-12 mr-4">
                            <b>
                            ((v(x+1,y)−v(x,y))−(s(x+1,y)−s(x,y)))**2<br>
                            ((v(x,y+1)−v(x,y))−(s(x,y+1)−s(x,y)))**2<br>
                             (v(1,1)−s(1,1))**2
                            </b>
                        </p>
                        <p class="col-xs-12 mr-4">
                            We use the equations to loop through each pixel of the source image to construct A and b. By solving the least square form of Av=b, we can get the synthesized image v. Say the given gray image size is H by W, A's dimension is <b>H*W</b> by <b>2*H*W+1</b>, b's dimension is <b>H*W</b> by <b>1</b>. The result is pretty much to test if we can copy the original image, as shown below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/toy.png" height="400px" alt="image_left">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            I implemented it in both loop method and non-loop method, the interesting thing is that with loop method, it only takes around 0.4s, whereas with the non-loop method, which is supposed to be faster in Python environment than loop, turns out to take around 10s. I think the major reason is that sparse matrix's arithmetic calculation is more expensive than directly assign value to coordinates. In the non-loop method, I mainly use lil_matrix to construct sparse matrix, and use np.roll, np.transpose to construct A matrix. Finally I decide to use the loop method for the rest of the task.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Poisson Blending</h3>
                        <p class="col-xs-12 mr-4">
                            Based on toy problem's hint, Poisson Blending explores the four neighbour of each pixel, follow the equation below that <b>v</b> is the synthesized vector that we need to solve, <b>s</b> is the source image in the size of target image(but we're only interested in the masked source image), <b>t</b> is the target image:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/poisson.png" height="80px" alt="image_left">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            In the equation, each <b>i</b> deals with 4 <b>j</b>, i.e. the same <b>i</b> is calculated 4 times with 4 different neighbour <b>j</b>. The left part considers the condition if all the neighbour of i is still inside the mask, and the right part considers the neighbour not all inside the mask. Notice the difference in code is that for the right part, <b>tj</b> is used to construct parameter <b>b</b>, whereas for the left part, <b>vj</b> is used to construct parameter <b>A</b>.
                        </p>
                        <p class="col-xs-12 mr-4">
                            Also, the given image now has RGB, 3 channels, therefore we need to calculate each channel separately. When I implemented it, I consider that <b>A</b> matrix is always the same for 3 channels, but <b>b</b> are different for each channel, therefore I only calculate <b>A</b> once and <b>b</b> 3 times to speed up the algorithm. What's more, since the we only need to generate image from the maksed source image's coordinate, I only loop through this area to speed up. And finally I merged three v together to get new RGB image. The average speed is related to the image size, to deal with the given example of <b>130x107</b>(source image) and <b>250x333</b>(taget image), it generally takes around 20s.
                        </p>
                        <p class="col-xs-12 mr-4">
                            However, the naive Poisson Blending has some issue to deal with blending the image seamlessly, which is because only considering the source neighbour is not enough for strong difference of target and source images. As you can see the image below, the inner part of the ballon house is little blur and not blended so well with the background. Therefore, we need to implement Mixed Blending.
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/house.jpg" height="150px">
                                <img src="./media/a2/house_mask.png" height="150px">
                                <img src="./media/a2/mountain.jpg" height="350px">
                                <img src="./media/a2/ballon_blend.png" height="600px">
                            </div>
                        </div>
                        <h2>Extra Credits</h2>
                        <h3>Mixed Gradients</h3>
                        <p class="col-xs-12 mr-4">
                            Mixed gradients is actually pretty straight forward based on Poisson Blending, we just add one condition that instead calculating the gradient of source image <b>s</b>, we compare the gradient of <b>s</b> and gradient of <b>t</b>, and take the larger one so that it can better blend when the difference of source and target image are large. The comparison of the same image can be seen as below(The left is blendered without Mixed gradients.):
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/ballon_blend.png" height="500px">
                                <img src="./media/a2/ballon.png" height="500px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            The given example of bear and pool is below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/source_01.jpg" height="150px">
                                <img src="./media/a2/source_01_mask.png" height="150px">
                                <img src="./media/a2/target_01.jpg" height="350px">
                                <img src="./media/a2/poisson_blend.png" height="600px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            There are some more generated blending images:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/obama.jpg" height="150px">
                                <img src="./media/a2/obama_mask.png" height="150px">
                                <img src="./media/a2/mona_lisa.jpg" height="300px">
                                <img src="./media/a2/obama.png" height="600px">
                            </div>
                        </div>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/swimmer.jpg" height="100px">
                                <img src="./media/a2/swimmer_mask.png" height="100px">
                                <img src="./media/a2/road.jpg" height="250px">
                                <img src="./media/a2/swimmer.png" height="600px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            And there is a failure case where the colorful Patrix can't blend so well with the gray-scale like moon image:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/patrick.jpg" height="100px">
                                <img src="./media/a2/patrick_mask.png" height="100px">
                                <img src="./media/a2/moon.jpg" height="200px">
                                <img src="./media/a2/patrick.png" height="600px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            This is mainly because there is upper limitation of the blending algorithm to adjust the color, if the source image and target image have too large difference, the algorithm will reach its limitation to find the solution that best approximate the least square function.
                        </p>
                        <h3>Color2Gray</h3>
                        <p class="col-xs-12 mr-4">
                            The Color2Gray method first turns RGB image to the HSV color space, and only consider the S and V channels to represent the color contrast and intensity respectively. In this way, we can keep the color contrast of rgb image and preserve the grayscale intensity at the same time. The algorithm runs similar to the Mixed Gradient where source and target image are S and V. The result is shown as below:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/colorBlindTest.png" height="300px">
                                <img src="./media/a2/color2gray.png" height="600px">
                            </div>
                        </div>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade show active" id="a3" role="tabpanel" aria-labelledby="a3-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #3</h2>
                        <hr class="col-xs-12 mr-4">
                        <h3>Introduction</h3>
                        <p class="col-xs-12 mr-4">
                            This project implements two famous GAN architecture: DCGAN and CycleGAN. It is programmed in Pytorch, the major code includes the build-up of discriminator and generator neural network, loss function, forward and backward propagations. It also explores different methods that help GAN generate better results, such as Data Agumentation, Differentiable Augmentation, variance of different lose functions, variance of different discriminators, and implemented in different dataset to check the robustness fo the network.
                        </p>
                        <hr class="col-xs-12 mr-4">
                        <h3>Part I: Deep Convulotional GAN</h3>

                        <p class="col-xs-12 mr-4"><b>Implement Data Augmentation</b></p>
                        <p class="col-xs-12 mr-4">
                            In Pytorch, data augmentation is invoked each time when iterated through the mini-batch, the purpose of the data augmentation is add variance to the dataset, which is especially useful when the dataset is small or each sample of the data are too similar. I mainly use the Resize + RandomCrop, RandomHorizontalFlip, RandomRotation(10 angles) to generate augmentation. The example of orignal dataset and augmented dataset, and GAN generated sample by different dataset can be see here:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a2/house.jpg" height="150px">
                                <img src="./media/a2/house_mask.png" height="150px">
                                <img src="./media/a2/mountain.jpg" height="350px">
                                <img src="./media/a2/ballon_blend.png" height="600px">
                            </div>
                        </div>
                        
                        <p class="col-xs-12 mr-4"><b>The Discriminator</b></p>
                        <p class="col-xs-12 mr-4">
                            The discriminator in this DCGAN is a convolutional neural network with the following architecture:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/discriminator.png" height="300px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            According to the formula <b>(N-F+2P)/S + 1 = M</b>(We don't consider dilation here), where <b>N</b> is the input number of channels, <b>M</b> is the output number of channels, <b>S</b> is stride, <b>P</b> is padding, <b>F</b> is size of filter/kernel, since we want <b>N=2M</b>, and we use filter size of 4, stride of 2, we can calculate that padding <b>P</b> is 1.
                            Besides, I use softmax for the output layer, and later on squared mean difference for loss function, since this Discriminator is a classification problem, the softmax-loss combination is a generally good choice.
                        </p>

                        <p class="col-xs-12 mr-4"><b>The Generator</b></p>
                        <p class="col-xs-12 mr-4">
                            The generator in this DCGAN is a convolutional neural network with the following architecture:
                        </p>
                        <div class="row mr-2">
                            <div class="text-center col-12">
                                <img src="./media/a3/generator.png" height="450px">
                            </div>
                        </div>
                        <p class="col-xs-12 mr-4">
                            In the generator neural network, I use transposed convolution with a filter, size of 4, stride of 1 and padding of 0 for the very first layer that from 100x1x1 input to 256x4x4 output. And the rest layer all are upsampling of 2, with a filter, size of 3, stride of 1 and padding of 1 to satisfy the condition that output dimension is the 2 times of input dimension. And the reason why the first layer is a transposed convolution is that it has better performance than direct upsampling for noise. 
                        </p>
                        <hr class="col-xs-12 mr-4">
                    </div>
                </div>
            </div>
            <div class="tab-pane fade" id="a4" role="tabpanel" aria-labelledby="a4-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #4</h2>
                        <hr class="col-xs-12 mr-4">
                        <p class="col-xs-12 mr-4">Lorem ipsum dolor sit amet, no eum reque putent scripta. Eos esse vidit nonumy eu. Sit ut nominati prodesset, affert consul expetendis et cum. Sea cu animal cotidieque. Ei qui affert voluptua hendrerit, eripuit menandri pericula et per. Eam cu iusto possim vocent, veri mazim sensibus ea has, salutandi periculis his eu. Mei tractatos sententiae no.
                            Eu vim eirmod molestie electram, eam ad dicunt facilisi conclusionemque, at nostrud delectus incorrupte mea. Vel ea liber munere maluisset, cu habeo cotidieque vel, blandit indoctum ei vel. Sit invidunt erroribus ne. Ea postea possit persecuti qui, tota dicit discere ut usu. Mei te iudicabit repudiare. Legere tritani definitiones vix ei, mea ea eros singulis.
                            Qui ne posidonium definitionem, maluisset repudiare appellantur eam et, vim et vivendum facilisi delicatissimi. Ius ei choro dictas. Sit in atqui antiopam, ut his iuvaret oportere sapientem. Solet equidem recteque eum at, nam mundi perpetua te.
                            Eum te semper appareat omittantur. Labitur perpetua ea mea, nam ea impetus partiendo. Elitr perfecto adipisci ut eum, his vocibus tincidunt incorrupte et. Ad duo putant tractatos. Sint case qualisque vis cu, soluta percipitur eu eam, cum inermis definitionem cu. Ne summo democritum pri, et falli ludus eruditi nam.
                            Per assum mucius ex, no nec petentium delicatissimi, et pri hinc tacimates similique. Purto prima omnes vel ad, eu feugiat nostrum eum, ut has idque laoreet periculis. Ad vel officiis lucilius accusata, nonumy volutpat te qui. Noster indoctum mediocritatem ne cum.</p>
                    </div>
                </div>
            </div>
            <div class="tab-pane fade" id="a5" role="tabpanel" aria-labelledby="a5-tab">
                <div class="row">
                    <div class="col-12 my-5 mx-3">
                        <h2>Assignment #5</h2>
                        <hr class="col-xs-12 mr-4">
                        <p class="col-xs-12 mr-4">Lorem ipsum dolor sit amet, no eum reque putent scripta. Eos esse vidit nonumy eu. Sit ut nominati prodesset, affert consul expetendis et cum. Sea cu animal cotidieque. Ei qui affert voluptua hendrerit, eripuit menandri pericula et per. Eam cu iusto possim vocent, veri mazim sensibus ea has, salutandi periculis his eu. Mei tractatos sententiae no.
                            Eu vim eirmod molestie electram, eam ad dicunt facilisi conclusionemque, at nostrud delectus incorrupte mea. Vel ea liber munere maluisset, cu habeo cotidieque vel, blandit indoctum ei vel. Sit invidunt erroribus ne. Ea postea possit persecuti qui, tota dicit discere ut usu. Mei te iudicabit repudiare. Legere tritani definitiones vix ei, mea ea eros singulis.
                            Qui ne posidonium definitionem, maluisset repudiare appellantur eam et, vim et vivendum facilisi delicatissimi. Ius ei choro dictas. Sit in atqui antiopam, ut his iuvaret oportere sapientem. Solet equidem recteque eum at, nam mundi perpetua te.
                            Eum te semper appareat omittantur. Labitur perpetua ea mea, nam ea impetus partiendo. Elitr perfecto adipisci ut eum, his vocibus tincidunt incorrupte et. Ad duo putant tractatos. Sint case qualisque vis cu, soluta percipitur eu eam, cum inermis definitionem cu. Ne summo democritum pri, et falli ludus eruditi nam.
                            Per assum mucius ex, no nec petentium delicatissimi, et pri hinc tacimates similique. Purto prima omnes vel ad, eu feugiat nostrum eum, ut has idque laoreet periculis. Ad vel officiis lucilius accusata, nonumy volutpat te qui. Noster indoctum mediocritatem ne cum.</p>
                    </div>
                </div>
            </div>
        </div>
        <!-- footer -->
        <div id="footer" class="text-center py-3">©2022 Copyright | <a href="http://caoyuchen.github.io" target="_blank">Joshua Cao</a> | yuchenca@andrew.cmu.edu</div>
    </div>
</body>

</html>
<!-- cloud effect -->
<!-- <script>
VANTA.CLOUDS2({
    el: "#cloud",
    mouseControls: true,
    touchControls: true,
    gyroControls: false,
    minHeight: 200.00,
    minWidth: 200.00,
    scale: 1.00,
    // skyColor: 0x6381b1,
    // cloudColor: 0x646493,
    speed: 1.50,
    texturePath: "./media/noise.png"
})
</script> -->
